# 可微分归纳逻辑编程

随着深度学习的兴起，神经网络的尺寸也伴随着表达力的提升而提升。随着神经网络复杂度的增加，过拟合问题也变得越来越普遍。虽然正则化可以缓解过拟合问题，但解决过拟合问题最常用的方法还是使用大量训练数据，希望足够多的训练数据的分布可以逼近测试数据的分布。然而，大量数据并不总是那么容易得到。相比神经网络，归纳逻辑编程可以说是极度数据高效了。然而，归纳逻辑编程对噪声和标注错误的数据极为敏感，也无法应用于非符号领域的问题，因此，归纳逻辑编程适用的领域比神经网络要狭窄得多。DeepMind的研究科学家Richard Evans和Edward Grefenstette提出了可微归纳逻辑编程∂ILP，结合了神经网络和归纳逻辑编程的优势。


DeepMind新研究：可微归纳逻辑编程，融汇神经网络与逻辑编程之长（上）

cloud.tencent.com/developer/news/82993

介绍了∂ILP如何将归纳推理问题转化为可满足性问题，以及∂ILP在标准ILP任务上的表现。

DeepMind新研究：可微归纳逻辑编程，融汇神经网络与逻辑编程之长（下）
cloud.tencent.com/developer/news/92628

将介绍∂ILP的可微架构，以及∂ILP对错误标注数据的鲁棒性和如何应用∂ILP到非符号数据。除此以外，我们还将讨论∂ILP与相关工作的异同，以及∂ILP的缺陷及改进空间。














----------------------------------------------------------------------------


相比**prolog的模式声明（mode declaration）**，和**metagol的元规则（meta rules）**，∂ILP的规则目标的限制要少得多。

为了克服传统ILP的缺陷，Garcez等在2015年尝试基于神经网络框架实现ILP系统，但是这一工作只在有限的概念验证性的例子上起效，未能成功应用于实际问题。

深度学习社区也尝试将ILP生成的符号程序表示改为隐式的基于网络权重分布的图灵机之类的底层计算模型。有不少工作基于这一思路，经此改造的ILP能够应对噪声和模糊数据。然而，和传统ILP不同，由于生成的计算模型过于底层，因此难以查探，不具可读性，而且概括性也很差，当测试数据包含很多训练数据中未见的新数据时，模型的表现剧烈下降。例如，假设训练数据长度为10，测试数据长度为20，这样的模型可能表现不错。但如果测试数据长度为100，模型的表现就急剧下降。

当然，也有一些工作采用了和∂ILP相近的思路。

Holldoble等在1999年证明，对任何特定类型的逻辑程序，存在一个循环神经网络，能够以任意所需精度逼近原逻辑程序。然而，这一工作仅仅是理论上的证明，并没有给出为具体程序构建循环神经网络的方法。（“Approximating the Semantics of Logic Programs by Recurrent Neural Networks”）

基于以上工作，Bader等在1999年给出了构造方法。（“Connectionist Model Generation: A First-Order Approach”）

该方法和∂ILP的不同点为：

Bader等考虑的是无环逻辑程序，而∂IL考虑的是Datalog逻辑程序。

由于使用网络的权重表达程序，因此难以查探和理解。

Bader等的网络的隐藏状态编码一个特定的基础原子集合，∂ILP中的隐藏状态编码重复应用的通用规则的相对权重。

Serafini和Garcez在2016年提出了“实逻辑”，将一阶逻辑中的布尔值替换为[0, 1]之间的实数值。这一做法与∂ILP类似。不过，如上篇所属，∂ILP能生成一阶逻辑无法表达的问题的解答。当然，“实逻辑”不像∂ILP一样限定了确定性Datalog子句。（“Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge”）

Bader、Garcez、Hitzler在2005年使用fibred神经网络表示逻辑程序。fibred神经网络可以表示递归程序，甚至互递归程序。然而，这一系统无法表示自由变量。（“Computing First-Order Logic Programs by Fibring Artificial Neural Networks”）

Kersting等在2006年改进了隐马尔科夫模型（HMM），提出了逻辑隐马尔科夫模型（LOHMM），该模型可以编码一阶表示。和∂ILP类似，LOHMM基于条件概率，可以应对噪声问题。和∂ILP不同，LOHMM基于原子序列进行学习，LOHMM的规则体只包含一个原子。（“Logical Hidden Markov Models”）

De Raedt和Kersting在2008年提出了一个非常概括的ILP形式化表示，基于覆盖（cover）关系（∂ILP的蕴涵关系属于覆盖关系中的一种）。不过，∂ILP可以学习递归子句，而这一系统无法学习递归子句。（“Probabilistic Inductive Logic Programming”）

Guillame-Bert、Broda、Garcez在2010年发表的论文“First-order Logic Learning in Artificial Neural Networks”使用了和∂ILP在很多方面相似的方法：同样限定确定性子句，使用神经网络架构实现正向链。和∂ILP不同，他们的方法没有Datalog限制（可以使用函数符号），然而，只能学习单个子句，也不支持递归子句。

Fran ̧ca、Zaverucha、Garcez在2014年提出的CILP++系统没有∂ILP的内存占用问题，但无法学习递归子句。总体而言，CILP++的设计方向与∂ILP不同，CILP++致力于学习更多数据，而∂ILP致力于学习更复杂的程序。（“Fast Relational Learning Using Bottom Clause Propositionalization with Artificial Neural Networks”）

Rockt ̈aschel和Riede在2016年提出了基于反向链的可微神经网络架构（∂ILP基于正向链）。另外，该系统的规则模板比∂ILP的限制更多。和∂ILP类似，该系统同样限定了Datalog子句。（“Learning Knowledge Base Inference with Neural Theorem Provers”）

Rockt ̈aschel和Riede在2016年提出了基于反向链的可微神经网络架构（∂ILP基于正向链）。另外，该系统的规则模板比∂ILP的限制更多。和∂ILP类似，该系统同样限定了Datalog子句。（“Learning Knowledge Base Inference with Neural Theorem Provers”）

Yang等在2016年提出了正向链的可微实现，和∂ILP不同，该实现学习单独子句，也不支持递归子句。（“A Differentiable Approach to Inductive Logic Programming”）

Eisner等在2004年提出了用于自然语言处理系统的Dyna语言，一个声明式一阶语言。和∂ILP类似，Dyna基于正向链推理计算概率。（“Dyna: A Declarative Language for Implementing Dynamic Programs”）

缺陷和改进空间

如前所述，∂ILP的缺陷主要是内存密集。这限制了∂ILP应用于大量数据，也难以应对需要三元以上谓词才能求解的问题。

目前∂ILP需要规则模板才能生成程序。研究人员尝试过使用暴力搜索法避免手工构造规则模板，然而，这牵涉大量运算。研究人员计划在以后使用更巧妙、更节省算力的解决方案。

另外，“处理模糊数据”一节已经提过，应用于模糊数据的∂ILP，可以改进的方面有：卷积神经网络与整个系统一起训练，根据手头的问题决定模糊数据的学习程度。
































