



# 1. TCS怎么解释神经网络效果好

顾险峰 拓扑解释 CV算法，信息论解释 老顾谈几何











https://qastack.cn/cstheory/36670/what-kind-of-answer-does-tcs-want-to-the-question-why-do-neural-networks-work-s





神经同调理论探索——用代数拓扑刻画神经网络容量 https://www.bilibili.com/video/BV1ns411E7rr


大脑与数学 第二部分——拓扑学中的对象：单纯复形 https://www.bilibili.com/video/BV1M5411573P



深度学习不再是炼丹术！谷歌给出首个神经网络训练理论证明 - 新智元的文章 - 知乎
https://zhuanlan.zhihu.com/p/57289737


从两个方面来问这个问题。

从网络结构上说，神经网络有五种典型结构：

前馈型网络（BP 网络属于这一支），有反馈前馈网络、前馈内层互联网络、反馈型全互联网络、反馈型局部互联网络。

从原理上说，神经网络有以下分支：

BP 神经网络、RBF 神经网络、Hopfield 神经网络、随机神经网络、遗传神经网络、粒子群神经网络、小波神经网络、模糊神经网络、混沌神经网络等等。



神经网络家族为何 BP 网络一枝独秀？谁能谈谈神经网络家族的兴衰史？ - 王源的回答 - 知乎
https://www.zhihu.com/question/364755843/answer/974861369



-------------------------------

神经网络为什么可以（理论上）拟合任何函数？ - 2prime的回答 - 知乎
https://www.zhihu.com/question/268384579/answer/1246730702

Yarotsky D. Error bounds for approximations with deep ReLU networks[J]. Neural Networks, 2017, 94: 103-114.
傅里叶神经网络

给定傅里叶:pdf,特定微分方程组，函数用nerual network怎么自动函数拟合？
已知布尔函数任何公式都可以用两层的神经网络去拟合，能不能用神经网络逻辑电路门“做出来一台虚拟机”？“神经网络机”，图灵完备计算“神经网络宇宙下的计算机”



神经网络为什么可以（理论上）拟合任何函数？ - 想飞的猫的回答 - 知乎
https://www.zhihu.com/question/268384579/answer/540793202


 



神经网络家族


早在1991年，已经有人用理论证明，如果神经网络的激活函数是连续、有界且非恒定值的，则可以在紧凑的输入集上实现连续映射。

说得通俗些，一个包含足够光滑激活函数的网络，能够以任意精确度逼近函数及其导数。也就是说，用神经网络可以找到三位问题中物体运动方程的近似解。


神经网络可以在一个紧致集（compact set）上逼近任意连续函数。

划重点，首先是紧致集，这是集合论中的知识，你可以把它想象为在一个确切的闭区间 [a, b] 内，可以用神经网络接近任何函数。这个边界必须要明确，实际上你是不可能使用神经网络对输入 x 在 [公式] 区间上逼近 [公式] 。



第二是逼近这个在数学上的定义，已经有答主列出了相关wiki：Universal approximation theorem，实际上逼近就是对于原函数 f(x) 来说，定义一个逼近函数函数的实现为F(x)，则对于任意小的误差 [公式] ，都有： [公式] 。数学上的证明你可以看这一篇，使用 sigmoid 进行函数逼近

Universal Approximation Bounds for Superpositions of a Sigmoidal Function


神经网络的从数学上是怎么证明可以无限逼近线性函数的？ - 代码律动的回答 - 知乎
https://www.zhihu.com/question/20684827/answer/392462120









目前对神经网络有哪些理论研究？ - dumaomao的回答 - 知乎
https://www.zhihu.com/question/265917569/answer/307096136


目前对神经网络有哪些理论研究？ - Ming的回答 - 知乎
https://www.zhihu.com/question/265917569/answer/301113087



神经网络家族为何 BP 网络一枝独秀？谁能谈谈神经网络家族的兴衰史？ - 王源的回答 - 知乎
https://www.zhihu.com/question/364755843/answer/974861369


神经网络为什么可以（理论上）拟合任何函数？ - 想飞的猫的回答 - 知乎
https://www.zhihu.com/question/268384579/answer/540793202

sigmoid函数


神经网络中的SIGMOID函数的意义？ - 黑怕的回答 - 知乎
https://www.zhihu.com/question/24259872/answer/623699200

为什么早期神经网络要采用sigmoid函数作为激活函数。

为了回答这个问题，首先我们稍微回顾一下神经网络背后的数学原理：

任何连续多元函数都能被一组一元函数的有限次叠加而成，其中每一个一元函数的自变量都是一组连续单变量函数的有限次加权叠加。而这内层的每一个单变量函数的自变量都是一个（即一维）变量。

举个例子，x*



这个数学原理被称为Kolmogorov-Arnold representation theorem是对于Hilbert第13问题的部分回答。
两层有限次叠加足够准确表示任何多元函数。这比多项式逼近要厉害多了，用多项式去准确表示一个连续多元（非多项式）函数需要无穷多项


1989年，George Cybenko在Approximation by Superpositions of a Sigmoidal Function 中提出了这一设想，并证明（存在性）只要一个隐藏层并使用sigmoidal-type函数就能一致逼近任意一个多元连续函数。不过隐藏层的单元数可能会非常大。这一结论不基于KST，而是基于Stone-Weierstrass定理。


如果你是在问为什么神经网络有这种逼近能力，直观解释请看: 
http://neuralnetworksanddeeplearning.com/chap4.html
 
试图理解其怎么工作的，可以看看:
http://cs231n.github.io/understanding-cnn/（包括其后的参考链接）
那么，为什么BP可以（保证？）找出我们想要的那组参数呢？据我所知，目前没有很好的数学解释。这里有一篇比较深入的论文:The Loss Surfaces of Multilayer Networks 
https://arxiv.org/pdf/1412.0233.pdf



用神经网络写算法
任意布尔公式，决策树分类算法用神经网络实现，多层perception感知机类:
经典数字图像处理算法，用CNN的傅里叶变换，图像算法，滤波，模糊，去噪声等等。通过这个解构，反思GNN为什么能生成图像，因为图像脸谱空间随机组合而已。
RNN,把NLP能不能从语义来解析？递归定义的语言结构，语法性质等编码为RNN,SLTM,记忆网络，attention机制，BERT网络…
二进制数字网络，数字电路模拟电路相关，能不能用网络去模拟？取整变成想要的功能的网络参数打印出来，从网络得到逻辑公式的方程表达式，然后那这个先验参考表达式进行逻辑推理证明，验证正确的话就是神经网络辅助EDA设计。


--------------------
递归定义神经网络中怎么实现？RNN吗
符号演算(多项式求导符号微积分)，定理证明，吴方法吴文俊几个证明等公式转网络？lambda演算，组合子等，符号推演公式转网络？

logic消解算法能不能用神经网络实现？




>深度学习，流形，拓扑
谱域分析
频域分析
图信号处理
GCN图神经网络
认知建模构图
tracktable才行，不稳定也不鲁棒不好
参数空间，非线性拟合+数学推演逻辑=可微分网络(第二代神经网络)


图信号。信号=不同空间的叠加，r进制的指数离散格状化2进制拓扑空间逼近连续，能量有效的子空间.


傅里叶变换具有卷积定理，和傅里叶基函数是拉普拉斯算子的特征函数，之间有什么关系？ - 方轩固的回答 - 知乎
https://www.zhihu.com/question/346245009/answer/826385011

可微分的流形，低维同胚


拉普拉斯的傅里叶变换的特征算子
为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基

Spectral Graph Theory图的谱分析

深入浅出图神经网络

图卷积网络(GCN)的谱分析 - XDU陈凌灏的文章 - 知乎
https://zhuanlan.zhihu.com/p/124727955



如何看待顾险峰《看穿机器学习(W-GAN模型)的黑箱》这篇文章？ - 张欣的回答 - 知乎
https://www.zhihu.com/question/55719494/answer/174113345


1. 如果真的从数学或者理论的角度来看深度学习，能在这方面作出贡献那真的是天才了，比如说Rice 的Richard Baranuik 解释Relu的那一套Maso，石溪顾险峰的那一套流形解释，CMU的Simon Du证明的一层全联接的全局最优。这些大佬们的工作真的是需要极高的数学门槛才能领悟的，非常值得尊敬。

2. 再比如陈天奇，李沐这一批最早开发Mxnet这些能行之有效easily-accessible深度学习工具的大佬们，也是非常牛逼非常值得尊敬的。

3.至于像题主所说的那种只会import **** as **** 的最底层调参民工并不能代表整个深度学习。至于被鄙视吗……其实感觉还挺理所应当的

https://mp.weixin.qq.com/s/5gyZqxhdvtH-zIxKini2TQ?
例如，人类首先发现了傅里叶分解原理，然后发现人类耳蜗神经结构就是在对声音信号进行傅里叶分解；又如，人类首先发现了保角变换（共形变换），后来发现从视网膜到第一级的视觉中枢就是保角变换


这几句有什么来源吗，只是好奇



我觉得这个讲神经网络“通用近似原理”既能“学习”又能“泛化”的视频很有启发性

https://b23.tv/iUh88I 学习观



这个系列把机器学习算法数学逻辑讲得超级明白

https://b23.tv/3ev5HX 白板系列

北大学霸写的简单粗暴tensorflow 教程也很强。https://tf.wiki/index.html
有人还对此进行视频讲解https://b23.tv/DNKSko

白板推导up主b站主页还有他更新的机器学习各种算法笔记链接





# 2. 总结：

刘博士的视频我觉得很有启发性，可以从XOR异或神经网络的逻辑公式等价关系出发


然后写到神经网络的公式角度理解：

1学习网络结构(函数空间)

2 学习给定结构的参数(梯度下降，函数空间内搜索参数)。

然后发现因为有了反向传播自动学习，神经网络的设计变成针对问题“设计对应求解算法函数map应该满足的因果链结构长相”，只要设计出来结构，就能学出来参数。所有目的变成设计能cover我们目标function的神经网络结构。

比如台湾老师讲的LSTM长短时记忆的机理，比如他讲的BERT/Attention,还有他讲的强化学习Deep NetWork改进网络结构......

又比如顾险峰从拓扑理解CV算法。

又比如前文链接 从拓扑，神经同调理论 理解人类大脑。


总之，只要设计能"cover可能需要的因果参数之间的公式关联"(同时拓扑结构，非线性公式函数同构)的非线性神经网络结构，然后自动学习网络参数就行!
























































































