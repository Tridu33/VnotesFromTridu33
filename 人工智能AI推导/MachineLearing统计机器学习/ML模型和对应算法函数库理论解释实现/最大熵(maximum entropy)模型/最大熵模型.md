




如何理解最大熵模型里面的特征？ - Semiring的回答 - 知乎
https://www.zhihu.com/question/24094554/answer/108271031


1.  **仅仅对输入抽取特征**。即特征函数为f(x)
2.  对输入和输出同时抽取特征。即特征函数为f(x,y)

  

要看清二者的关系，一个简单的办法就是去考察题主提到的**最大熵模型**和 **logistic 回归模型**。确切地说，看看怎么把最大熵模型推导成 logistic 回归模型就可以了。





# 最大熵模型

最大熵原理: 概率模型学习的一个准则, 最大熵原理认为, 学习概率模型时, 在所有可能的概率模型中, 熵最大的模型是最好的模型. 通常用约束条件来确定概率模型的集合, 所以, 最大熵原理也可以表述为在满足约束条件的模型集合中选择熵最大的模型. 



[一文搞懂HMM（隐马尔可夫模型）](https://www.cnblogs.com/skyme/p/4651331.html)


[[白话解析] 深入浅出最大熵模型](https://www.cnblogs.com/rossiXYZ/p/12244760.html)

最大熵模型里的熵是怎么定义的.

准确地说, 是模型P(Y | X)与经验分布P(X)的条件熵. 也就是:

可以理解为模型在当前样本的特征分布下预测结果的熵, 熵越大, 预测结果在各个类之间分布越均匀.








学习方法:

最原始的最大熵模型的训练方法是一种称为通用迭代算法GIS(generalized iterative scaling) 的迭代 算法。GIS 的原理并不复杂，大致可以概括为以下几个步骤：

1\. 假定第零次迭代的初始模型为等概率的均匀分布。

2\. 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布，如果超过了实际的，就把相应的模型参数变小；否则，将它们便大。

3\. 重复步骤 2 直到收敛。

GIS 算法每次迭代的时间都很长，需要迭代很多次才能收敛，而且不太稳定.

八十年代，很有天才的孪生兄弟的达拉皮垂(Della Pietra)在 IBM 对 GIS 算法进行了两方面的改进，提出了改进迭代算法 IIS（improved iterative scaling）。这使得最大熵模型的训练时间缩短了一到两个数量级。这样最大熵模型才有可能变得实用。即使如此，在当时也只有 IBM 有条件是用最大熵模型。


最大熵模型 - 严力的文章 - 知乎
https://zhuanlan.zhihu.com/p/36012167





《统计学习方法》最大熵模型推导注解 - 霖霖霖丶的文章 - 知乎
https://zhuanlan.zhihu.com/p/59519202







































































