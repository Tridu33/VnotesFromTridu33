


https://blog.csdn.net/qq_35082030/article/details/73368962 一文看懂25个神经网络模型
25种神经结构


>神经元

MLP多层感知机


卷积神经元（Convolutional cells）


解卷积神经元

池化神经元和插值神经元（Pooling and interpolating cells）

均值神经元和标准方差神经元（Mean and standard deviation cells）（作为概率神经元它们总是成对地出现）

循环神经元（Recurrent cells ）

长短期记忆神经元（Long short term memory cells）



门控循环神经元（Gated recurrent units (cells)）
是LSTM的变体。



神经细胞层(Layers)

卷积连接层（Convolutionally connected layers）

时间滞后连接（Time delayed connections）


>前馈神经网络（FFNN）

前馈神经感知网络与感知机（FF or FFNN：Feed forward neural networks and P：perceptrons）非常简单，信息从前往后流动（分别对应输入和输出）。



>径向基神经网络（RBF）

径向神经网络（RBF：Radial basis function）是一种以径向基核函数作为激活函数的前馈神经网络。没有更多描述了。这不是说没有相关的应用，但大多数以其它函数作为激活函数的FFNNs都没有它们自己的名字。这或许跟它们的发明年代有关系。

>霍普菲尔网络（HN）

霍普菲尔网络（HN：Hopfield network）是一种每一个神经元都跟其它神经元相互连接的网络。


>马尔可夫链（MC）

马尔可夫链（MC：Markov Chain）或离散时间马尔可夫链（DTMC：MC or discrete time Markov Chain）在某种意义上是BMs和HNs的前身。可以这样来理解：从从我当前所处的节点开始，走到任意相邻节点的概率是多少呢？它们没有记忆（所谓的马尔可夫特性）：你所得到的每一个状态都完全依赖于前一个状态。尽管算不上神经网络，但它却跟神经网络类似，并且奠定了BM和HN的理论基础。跟BM、RBM、HN一样，MC并不总被认为是神经网络。此外，它也并不总是全连接的。

>玻尔兹曼机（BM）

玻尔兹曼机（BM：Boltzmann machines）和霍普菲尔网络很接近，差别只是：一些神经元作为输入神经元，剩余的则是作为隐神经元。

>受限玻尔兹曼机（RBM）


受限玻尔兹曼机（RBM：Restricted Boltzmann machines）与BM出奇地相似，因而也同HN相似。

它们的最大区别在于：RBM更具实用价值，因为它们受到了更多的限制。它们不会随意在所有神经元间建立连接，而只在不同神经元群之间建立连接，因此任何输入神经元都不会同其他输入神经元相连，任何隐神经元也不会同其他隐神经元相连。

RBM的训练方式就像稍微修改过的FFNN：前向通过数据之后再将这些数据反向传回（回到第一层），而非前向通过数据然后反向传播误差。之后，再使用前向和反向传播进行训练。



>自编码机（AE）

自编码机（AE：Autoencoders）和FFNN有些相近，因为它更像是FFNN的另一种用法，而非本质上完全不同的另一种架构。
自编码机的基本思想是自动对信息进行编码（像压缩一样，而非加密），它也因此而得名。整个网络的形状酷似一个沙漏计时器，中间的隐含层较小，两边的输入层、输出层较大。自编码机总是对称的，以中间层（一层还是两层取决于神经网络层数的奇偶）为轴。最小的层（一层或者多层）总是在中间，在这里信息压缩程度最大（整个网络的关隘口）。在中间层之前为编码部分，中间层之后为解码部分，中间层则是编码部分。


>稀疏自编码机（SAE）

稀疏自编码机（SAE：Sparse autoencoders）某种程度上同自编码机相反。稀疏自编码机不是用更小的空间表征大量信息，而是把原本的信息编码到更大的空间内。因此，中间层不是收敛，而是扩张，然后再还原到输入大小。它可以用于提取数据集内的小特征。

如果用训练自编码机的方式来训练稀疏自编码机，几乎所有的情况，都是得到毫无用处的恒等网络（输入=输出，没有任何形式的变换或分解）。为避免这种情况，需要在反馈输入中加上稀疏驱动数据。稀疏驱动的形式可以是阈值过滤，这样就只有特定的误差才会反向传播用于训练，而其它的误差则被忽略为0，不会用于反向传播。这很像脉冲神经网络（并不是所有的神经元一直都会输出）。


>变分自编码机（VAE）

变分自编码机（VAE：Variational autoencoders）和AE有着相同的架构，却被教会了不同的事情：输入样本的一个近似概率分布，这让它跟BM、RBM更相近。

不过，VAE却依赖于贝叶斯理论来处理概率推断和独立（probabilistic inference and independence），以及重新参数化（re-parametrisation）来进行不同的表征。推断和独立非常直观，但却依赖于复杂的数学理论。基本原理是：把影响纳入考虑。如果在一个地方发生了一件事情，另外一件事情在其它地方发生了，它们不一定就是关联在一起的。如果它们不相关，那么误差传播应该考虑这个因素。这是一个有用的方法，因为神经网络是一个非常大的图表，如果你能在某些节点排除一些来自于其它节点的影响，随着网络深度地增加，这将会非常有用。


>去噪自编码机（DAE）

去噪自编码机（DAE：Denoising autoencoders）是一种自编码机，它的训练过程，不仅要输入数据，还有再加上噪音数据（就好像让图像变得更加模糊一样）。
但在计算误差的时候跟自动编码机一样，降噪自动编码机的输出也是和原始的输入数据进行对比。这种形式的训练旨在鼓励降噪自编码机不要去学习细节，而是一些更加宏观的特征，因为细微特征受到噪音的影响，学习细微特征得到的模型最终表现出来的性能总是很差。



>深度信念网络（DBN）

深度信念网络（DBN：Deep belief networks）之所以取这个名字，是由于它本身几乎是由多个受限玻尔兹曼机或者变分自编码机堆砌而成。实践表明一层一层地对这种类型的神经网络进行训练非常有效，这样每一个自编码机或者受限玻尔兹曼机只需要学习如何编码前一神经元层的输出。这种训练技术也被称为贪婪训练，这里贪婪的意思是通过不断地获取局部最优解，最终得到一个相当不错解（但可能不是全局最优的）。可以通过对比散度算法或者反向传播算法进行训练，它会慢慢学着以一种概率模型来表征数据，就好像常规的自编码机或者受限玻尔兹曼机。一旦经过非监督式学习方式，训练或者收敛到了一个稳定的状态，那么这个模型就可以用来产生新的数据。如果以对比散度算法进行训练，那么它甚至可以用于区分现有的数据，因为那些神经元已经被引导来获取数据的不同特定。



>卷积神经网络（CNN）


卷积神经网络（CNN：Convolutional neural networks）或深度卷积神经网络（DCNN：deep convolutional neural networks）跟其它类型的神经网络大有不同。它们主要用于处理图像数据，但可用于其它形式数据的处理，如语音数据。对于卷积神经网络来说，一个典型的应用就是给它输入一个图像，而后它会给出一个分类结果。也就是说，如果你给它一张猫的图像，它就输出“猫”；如果你给一张狗的图像，它就输出“狗”。



>解卷积网络（DN）

解卷积网络（DN：Deconvolutional networks），又称为逆图形网络（IGNs：inverse graphics networks），是逆向的卷积神经网络。
想象一下，给一个神经网络输入一个“猫”的词，就可以生成一个像猫一样的图像，通过比对它和真实的猫的图片来进行训练。跟常规CNN一样，DN也可以结合FFNN使用，但没必要为这个新的缩写重新做图解释。它们可被称为深度解卷积网络，但把FFNN放到DNN前面和后面是不同的，那是两种架构（也就需要两个名字），对于是否需要两个不同的名字你们可能会有争论。需要注意的是，绝大多数应用都不会把文本数据直接输入到神经网络，而是用二元输入向量。比如<0,1>代表猫，<1,0>代表狗，<1,1>代表猫和狗。


>深度卷积逆向图网络（DCIGN）

深度卷积逆向图网络（DCIGN：Deep convolutional inverse graphics networks），这个名字具有误导性，因为它们实际上是VAE，但分别用CNN、DNN来作编码和解码的部分。
这些网络尝试在编码过程中对“特征“进行概率建模


> 生成式对抗网络（GAN）


生成式对抗网络（GAN：Generative adversarial networks）是一类不同的网络，它们有一对“双胞胎”：两个网络协同工作。
GAN可由任意两种网络组成（但通常是FF和CNN），其中一个用于生成内容，另一个则用于鉴别生成的内容。
鉴别网络（discriminating network）同时接收训练数据和生成网络（generative network）生成的数据。鉴别网络的准确率，被用作生成网络误差的一部分。这就形成了一种竞争：鉴别网络越来越擅长于区分真实的数据和生成数据，而生成网络也越来越善于生成难以预测的数据。这种方式非常有效，部分是因为：即便相当复杂的类噪音模式最终都是可预测的，但跟输入数据有着极为相似特征的生成数据，则很难区分。

训练GAN极具挑战性，因为你不仅要训练两个神经网络（其中的任何一个都会出现它自己的问题），同时还要平衡两者的运行机制。如果预测或生成相比对方表现得过好，这个GAN就不会收敛，因为它会内部发散。



> 循环神经网络（RNN）




循环神经网络（RNN：Recurrent neural networks）是具有时间联结的前馈神经网络：它们有了状态，通道与通道之间有了时间上的联系。神经元的输入信息，不仅包括前一神经细胞层的输出，还包括它自身在先前通道的状态。

这就意味着：你的输入顺序将会影响神经网络的训练结果：相比先输入“曲奇饼”再输入“牛奶”，先输入“牛奶”再输入“曲奇饼”后，或许会产生不同的结果。RNN存在一大问题：梯度消失（或梯度爆炸，这取决于所用的激活函数），信息会随时间迅速消失，正如FFNN会随着深度的增加而失去信息一样。

直觉上，这不算什么大问题，因为这些都只是权重，而非神经元的状态，但随时间变化的权重正是来自过去信息的存储；如果权重是0或1000000，那之前的状态就不再有信息价值。

原则上，RNN可以在很多领域使用，因为大部分数据在形式上不存在时间线的变化，（不像语音或视频），它们能以某种序列的形式呈现出来。一张图片或一段文字可以一个像素或者一个文字地进行输入，因此，与时间相关的权重描述了**该序列前一步**发生了什么，而不是多少秒之前发生了什么。一般来说，循环神经网络是推测或补全信息很好的选择，比如自动补全。


>长短期记忆（LSTM）


长短期记忆（LSTM：Long / short term memory）网络试图通过引入门结构与明确定义的记忆单元来解决梯度消失/爆炸的问题。
这更多的是受电路图设计的启发，而非生物学上某种和记忆相关机制。

每个神经元都有一个记忆单元和三个门：输入门、输出门、遗忘门。这三个门的功能就是通过禁止或允许信息流动来保护信息。
输入门决定了有多少前一神经细胞层的信息可留在当前记忆单元，输出层在另一端决定下一神经细胞层能从当前神经元获取多少信息。遗忘门乍看很奇怪，但有时候遗忘部分信息是很有用的：比如说它在学习一本书，并开始学一个新的章节，那遗忘前面章节的部分角色就很有必要了。
实践证明，LSTM可用来学习复杂的序列，比如像莎士比亚一样写作，或创作全新的音乐。值得注意的是，每一个门都对前一神经元的记忆单元赋有一个权重，因此会需要更多的计算资源。



>门循环单元（GRU）


门循环单元（GRU : Gated recurrent units）是LSTM的一种轻量级变体。它们少了一个门，同时连接方式也稍有不同：它们采用了一个更新门（update gate），而非LSTM所用的输入门、输出门、遗忘门。

更新门决定了保留多少上一个状态的信息，还决定了收取多少来自前一神经细胞层的信息。重置门（reset gate）跟LSTM遗忘门的功能很相似，但它存在的位置却稍有不同。它们总是输出完整的状态，没有输出门。多数情况下，它们跟LSTM类似，但最大的不同是：GRU速度更快、运行更容易（但函数表达力稍弱）。

在实践中，这里的优势和劣势会相互抵消：当你你需要更大的网络来获取函数表达力时，这样反过来，性能优势就被抵消了。在不需要额外的函数表达力时，GRU的综合性能要好于LSTM。





>神经图灵机（NTM）

神经图灵机（NTM: Neural Turing machines）可以理解为对LSTM的抽象，它试图把神经网络去黑箱化（以窥探其内部发生的细节）。
NTM不是把记忆单元设计在神经元内，而是分离出来。NTM试图结合**常规数字信息存储**的高效性、永久性与**神经网络的效率及函数表达能力**。
它的想法是设计一个可作内容寻址的记忆库(*无限长纸带一般等价物*)，并让神经网络对其进行读写操作。NTM名字中的“图灵（Turing）”是表明，它是**图灵完备（Turing complete）**的，即具备基于它所读取的内容来读取、写入、修改状态的能力，也就是能表达一个通用图灵机所能表达的一切。






>BiRNN、BiLSTM、BiGRU

双向循环神经网络（BiRNN：Bidirectional recurrent neural networks）、双向长短期记忆网络（BiLSTM：bidirectional long / short term memory networks ）和双向门控循环单元（BiGRU：bidirectional gated recurrent units）在图表中并未呈现出来，因为它们看起来与其对应的单向神经网络结构一样。
所不同的是，这些网络不仅与**过去的状态有连接，而且与未来的状态也有连接**。比如，通过一个一个地输入字母，训练单向的LSTM预测“鱼（fish）”（在时间轴上的循环连接记住了过去的状态值）。在BiLSTM的反馈通路输入序列中的下一个字母，这使得它可以了解未来的信息是什么。这种形式的训练使得该网络可以填充信息之间的空白，而不是预测信息。因此，它在处理图像时不是扩展图像的边界，而是填补一张图片中的缺失。



>深度残差网络（DRN）


深度残差网络（DRN: Deep residual networks）是非常深的FFNN网络，它有一种特殊的连接，可以把信息从某一神经细胞层传至后面几层（通常是2到5层）。
该网络的目的不是要找输入数据与输出数据之间的映射(*判别模型*)，而是致力于构建**输入数据与输出数据+输入数据之间的映射函数**(*生成模型*)。本质上，它在结果中增加一个恒等函数，并跟前面的输入一起作为后一层的新输入。结果表明，当层数超过150后，这一网络将非常擅于学习模式，这比常规的2到5层要多得多。然而，有证据表明这些网络*本质上只是没有时间结构的RNN*，它们总是与没有门结构的LSTM相提并论。




>回声状态网络（ESN）


回声状态网络（ESN：Echo state networks）是另一种不同类型的（循环）网络。

它的不同之处在于：神经元之间的连接是随机的（没有整齐划一的神经细胞层），其训练过程也有所不同。不同于输入数据后反向传播误差，ESN先输入数据、前馈、而后更新神经元状态，最后来观察结果。它的输入层和输出层在这里扮演的角色不太常规，输入层用来主导网络，输出层作为激活模式的观测器随时间展开。在训练过程中，只有观测和隐藏单元之间连接会被改变。




>极限学习机（ELM）

极限学习机（ELM：Extreme learning machines）本质上是拥有随机连接的FFNN。
它们与LSM、ESN极为相似，除了循环特征和脉冲性质，它们还不使用反向传播。相反，它们先给权重设定随机值，然后根据最小二乘法拟合来一次性训练权重（在所有函数中误差最小）。这使ELM的函数拟合能力较弱，但其运行速度比反向传播快多了。


>液态机（LSM）


液态机（LSM：Liquid state machines）换汤不换药，跟ESN同样相近。
区别在于，LSM是一种脉冲神经网络（spiking neural networks），用阈值激活函数（threshold functions）取代了sigmoid激活函数，每个神经元同时也是具有累加性质的记忆单元。因此，当神经元状态更新时，其值不是相邻神经元的累加值，而是它自身状态值的累加。一旦累加到阈值，它就释放能量至其它神经元。这就形成了一种类似于脉冲的模式：神经元不会进行任何操作，直至到达阈值的那一刻。



>支持向量机（SVM）

支持向量机（SVM：Support vector machines）能为分类问题找出最优方案。

传统意义上，它们只能处理**线性可分**的数据；比如找出哪张图片是加菲猫、哪张是史努比，此外就无法做其它输出了。

训练过程中，SVM可以理解为：先在平面图表上标绘所有数据（加菲猫、史努比），然后找出到那条能够最好区分这两类数据点的线。这条线能把数据分为两部分，线的这边全是史努比，线的那边全是加菲猫。而后移动并优化该直线，令两边数据点到直线的距离最大化。分类新的数据，则将该数据点画在这个图表上，然后察看这个数据点在分隔线的哪一边（史努比一侧，还是加菲猫一侧）。

通过使用**核方法**，SVM便可用来分类n维空间的数据。这就引出了在3维空间中标绘数据点，从而让SVM可以区分史努比、加菲猫与西蒙，甚至在更高的维度对更多卡通人物进行分类。SVM并不总被视为神经网络。




>Kohonen 网络

Kohonen网络（KN，也称之为自组织（特征）映射（SOM/SOFM：self organising (feature) map））。
KN利用竞争学习来对数据进行分类，不需要监督。先给神经网络一个输入，而后它会评估哪个神经元最匹配该输入。然后这个神经元会继续调整以更好地匹配输入数据，同时带动相邻的神经元。相邻神经元移动的距离，取决于它们与最佳匹配单元之间的距离。KN有时也不被认为是神经网络。





https://www.asimovinstitute.org/neural-network-zoo/ 对应github原仓库
DL之NN：目前最全、最完整的Neural Networks中，各种算法系统原理结构图



























# 1. aicheatsheets








http://www.aicheatsheets.com/

https://github.com/Niraj-Lunavat/Artificial-Intelligence
Awesome AI Learning with +100 AI Cheat-Sheets, Free online Books, Top Courses, Best Videos and Lectures, Papers, Tutorials, +99 Researchers, Premium Websites, +121 Datasets, Conferences, Frameworks, Tools

出色的AI学习与+100 aicheatsheets，免费在线书籍，热门课程，最佳视频和讲座，论文，教程，+ 99研究人员，高级网站，+ 121数据集，会议，框架，工具

# 2. AI-BigData-Cheatsheets

https://github.com/kevalds51/AI-BigData-Cheatsheets























