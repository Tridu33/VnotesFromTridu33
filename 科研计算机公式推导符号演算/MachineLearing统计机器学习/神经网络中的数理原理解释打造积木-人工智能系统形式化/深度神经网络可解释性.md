# 1. 神经网络数理可解释性

[TOC]


@Author:tridu33@qq.com



## 1.1. Introduction

### 1.1.1. 神经网络仿生解释

可解释性：Interpretability或者Interpretable AI是使本就透明的模型（白盒）通俗易懂，所有经验水平的用户都可以明白模型的含义；而Explainabilily或者Explainable AI则是研究者对于黑盒模型的行为作出人们所能理解的解释。本文讨论是包括这两种理论的更广义一些的可解释性。

先谈点生物，从生物进化学出发探究为何生物演化出神经网络？

神经网络：同时有记忆功能和决策推理功能的最佳选择，更重要的是为防止信息不足带来训练时期的过拟合，决策结构有遗忘机制。能通过修改神经细胞群的链接强弱能力，重新建立新旧链接等机制来更新迭代现有知识。这叫做”神经网络细胞群的自然选择“，可以解释在生物寿命周期内的演化和学习。

假如已知一个具体算法函数公式的话，决策结构应该怎么设计对应功能的神经网络结构？如果先有算法再设计神经网络具体可以怎么做呢？

先搭一个多层网络，不需要知道具体映射，不需要知道具体参数大小，只需要知道大概解函数映射中，大概需要”谁和谁建立联系，谁和谁有关联”，设计神经网络结构初始训练函数族的时候就要cover这种相关关系。这很重要！

换句话说，神经网络学习设计中，利用可解释性，在特定问题设计时，针对该问题数据概率分布特点，分析问题解映射的特点，数据关联，去设计能cover这种特点的最适合的网络结构，去train自动获取参数(就像turing的P/A/B型机器内生计算)。

### 1.1.2. 为什么要研究可解释性


我理解研究神经网络可解释性的核心动力：可解释性不是为让人“以为自己理解”，不是为让人心里觉得爽或者无意义不可靠的试验来battle(当然，可靠思辨有意义的实验设计验证很重要)，而是要通过理解进而指导生产生活，引导设计，设计的超参给定网络函数组要能cover对问题对应数据分布的解空间任一函数！

神经网络是一个可微函数族，那就让这个函数族能覆盖任务的特点；关键应该是结构的设计，所谓结构的设计，要遵循的原则就是函数族覆盖问题的特性。

可解释性是为能指导生产生活，比如根据现有CV经典算法公式程序，能不能设计出可解释性的等效网络，甚至泛用性更强，于是运算更快的BNN之类的。比如整理好不同网络结构cover不同特性的任务，然后在此基础上算法网络结构搭积木，从而批量化排列组合生成可靠有效的神经网络产品，[25种神经结构和对应github那个仓库](https://blog.csdn.net/qq_35082030/article/details/73368962 )。

大体分两类网络结构：”特征提取/知识表示类”；”决策/回归/分类/推理类”。

重申核心观点，可理解性的目标是指导生产：设计的超参给定网络函数组要能cover对问题对应数据分布的解空间任一函数.
既然神经网络是一个可微函数族，那就让这个函数族能覆盖任务的特点；关键应该是结构的设计，所谓结构的设计，要遵循的原则就是函数族覆盖问题的特性。

下面是一些例子解释：

《数字图像处理》中学到：空间域卷积=频域Fourier变换，本质就是特征提取滤波。若是高通滤波器（则，边缘提取）/ 低通录波器（图像平滑，提取整体特征）。傅里叶变换就是把一个任意函数（序列）转化为一个个不同频率的三角函数（序列）的和的函数“裂解器”。Fourier变换提出背景：为对复杂函数进行偏微分计算。1822年发表的《热的解析理论》，直觉上就跟神经网络数学结构，流形，局部欧几里得可微可导保证BP正确性，实现顾险峰口中的共形变换等密切相关(更多数学细节不敢妄言)。共形变换保持角度不变的映射。共形变换保持角度以及无穷小物体的形状，但是不一定保持它们的尺寸。共形的性质可以用坐标变换的导数矩阵雅可比矩阵的术语来表述。如果变换的雅可比矩阵处处都是一个标量乘以一个旋转矩阵，则变换是共形的。


图像识别任务：cnn识别类网络+mlp多层感知机推理分类回归决策网络 不一定就是最优，
其中识别网络可换成别的，推理网络也可换成别的，排列组合就能水出很多刷榜论文。

比如《Training decision trees as replacement for convolution layers》用像素概率分布决策图替换卷积实现图像分类的任务，感兴趣也可看知乎链接关于不是用[神经网络之前手写识别是怎么做的](https://www.zhihu.com/question/29238666/answer/232523424)？概率分布。

《特征工程》word2vec训练出来的词的embedding不但实现NLP词的量化为计算机可读数据格式，后来还发展glove表示context和语义知识。

Seq2seq，数据分布特点是节点和上一个节点有关系(时序性),这实际上在结构上进行递归的神经网络计算图。较早的RNN连乘一直带来两个问题：梯度爆炸和消失。在前向过程中，开始时刻的输入对后面时刻的影响越来越小，这就是长距离依赖问题。这样一来，就失去“记忆”的能力。要知道生物的神经元拥有对过去时序状态很强的记忆能力。

长短期记忆（Long short-term memory, LSTM）就是要解决这两个问题，通过引入若干门来解决，相比RNN多一个状态cell state。cell state承载着之前所有状态的信息，每到新的时刻，就有相应的操作来决定舍弃什么旧的信息以及添加什么新的信息。这个状态与隐藏层状态h不同，在更新过程中，它的更新是缓慢的（从而实现比较长的短时记忆），而隐藏层状态h的更新是迅速的。

[DQN强化学习例子](https://daiwk.github.io/posts/rl-stepbystep-chap13.html)：

需要识别sensor？值迭代过程用CNN嵌入到策略网络中
需要建模序列状态？那就网络迭代k次，理解为看k步后的值函数
值迭代网络需要输入特征局部加权？那就Attention机制。

人工智能有三种记忆方式：

1感知记忆：比如前文提到的特征感知类网络

首先发现傅里叶分解原理，然后发现人类耳蜗神经结构就是在对声音信号进行傅里叶分解；
人类首先发现保角变换（共形变换），后来发现从视网膜到第一级的视觉中枢就是保角变换。

2短时记忆working记忆

3长时记忆，提取和当前知识相关内容。

Attention局部加权，强者恒强，RNN/LSTM问题是memory 没法太大，因为随着memory参数k增加k by k matrix增加很快.参数太多太容易overfitting，于是设计Attention参数增加数据不会增太快，类似不同卷积核不同特征，多头多个特征。类比MLP参数太多空间指数碰撞过拟合，cov卷积共享参数防止过拟合。
实际上Attention可以看作在seq2seq在RNN/LSTM难以并行计算基础上，模仿CNN并行计算的一种共享参数防止”参数过多过拟合”的设计。目标是对context上下文加权，关注局部细节的作用。Self attention自注意力保持。[《Show, Attend and Tell: Neural Image CaptionGeneration with Visual Attention》](https://github.com/ryankiros/neural-storyteller)

### 1.1.3. 数据概率分布解空间对应的决策结构

其次，引发本文调研分析的核心指导思想是这个观点：decision structure 才是解决问题正确的控制流图。

决策函数(二分类器/直线)边界分割方式可以有无数种(线段偏移)，只要线性层train出来的能够被激活函数正确地做出决策二分类，此基础上组合网络结构，就是能学到解决对应问题的函数映射等效的decision structure。

> NN中线性代数计算部分只是为计算权重，实际上非线性激活函数才是每个网络cell投票决策的能力，并且还起到特征选择的作用。例如，经[Rectified Linear Unit, ReLU抑制为0的神经元在之后的学习过程中不起任何作用](https://blog.csdn.net/weixin_38286298/article/details/90319134)。 因此用relu激活函数更适合深度网络的投票/决策/推理/判断/分类/回归类网络。决策**decision structure**才是核心功能算法映射，反向BP梯度下降训练的参数只是用来判断当前cell能否达到选用的激活函数做出正确投票分类的决策阈值。

> 关于激活函数和decision structure的关联：异或门用的是thresh阈值激活函数，那么如果换成其他激活函数呢？早在1991年，已经有人用理论证明，如果[神经网络的激活函数是连续、有界且非恒定值的，则可以在紧凑的输入集上实现连续映射](https://www.zhihu.com/question/24259872/answer/623699200),最开始人们以为只有sigmiod有这样特性，后来发现只要是连续映射KST条件激活函数就能投票。simoid不是必须的(梯度消失问题)，甚至现在基本总用Relu，多快好省方便利索。假设换激活函数后，只是每个cell决策阈值量化的自变量数值上看起来不一样，其实换个激活函数，决策网络decition structure结构也和异或门决策结构类似。这也能从函数角度解释为什么这么多XOR异或门的等效实现。那么一个重要的观察结论就是：(因为当前异或门计算任务的数据概率分布解空间有无数个，神经网络只要概率性fitting 解空间一个就行，而不需要得到确定性求解算法表达式)。这点文末进一步解释。

> 线性代数运算还有一个好处：连续可微可导，可以反向求导自动求出来的(当前选用的连续非线性激活函数决策阈值)的参数权重，来适应对应激活函数的决策阈值。这样子只需要设计出结构，就能暴力硬train一发得到决策过程无脑调参! 。只要设计出的神经网络结构有着能够cover到“问题任务需求的数据概率性分布规律”解空间 的可能性，然后BP自动train出来的解结构就能解决问题。

decision structure 才是解决问题正确的控制流图,认为决策图结构是类似相同目标函数对应算法计算图的，具体体现就是train出来类似的适应当前问题特定数据概率分布规律的决策网络。支撑这个观点的一个证据是：[二值网络能力](https://zhuanlan.zhihu.com/p/270184068)也不算差。一方面，[二值网络(起因)压缩技术实现更高效的推理](https://zhuanlan.zhihu.com/p/350439652),这就很自然联想到SMT-lib。另一方面，可以利用二值网络来进一步研究卷积神经网络的可解释性，即找出哪一层是重要的，哪一层是无用的并且可以从黑盒中删除，哪种结构有利于准确预测。很多时候，train出来的网络结构，去掉dead cell/dead substructure后的子图主要走特定的结构。

### 1.1.4. Curry-Howard-Lambek correspondence

一个decision structure解，如果理解为一个tensorflow算法代码实现的话，可以画出计算图，结构化程序定理告诉决策结构可以用一个计算图，一个decision structure自动机迁移系统来等效表示。

结构化程序定理:“任意正规程序都能等价于基集合{序列，if then rules, while do}产生的结构化程序，可以理解为程序(函数)流程图。程序算法本质同构画出等效控制流图/FSC/Grammar。迁移系统(程序/算法/逻辑功能电路验证/设计运行系统(电梯系统PLC画图程序之类的)/设计的协议是否能满足LTL目标使用需求/自动机/确定性策略的Markov Network/状态迁移图...

AutoGraph 将函数中的 Python 控制流语句转换成 TensorFlow 计算图中的对应节点。如 while 和 for 语句转换为 tf.while ， if 语句转换为 tf.cond 等。

假设把这个算法语义结构翻译成霍尔逻辑，决策结构算法决策步骤就是霍尔三元组描述的符号演算逻辑推理规则中，把问题抽象成等效文法问题/符号逻辑推理/归纳逻辑ILP规则学习问题。

神经网络可以表示任意的布尔表达式。那决策结构文法规则也就能用网络结构表达式。但是因为不方便等原因，实际上决策结构还是直接从自动机角度理解就好。Floyd–Hoare logic用严格的数理逻辑推理来保障程序正确性，比如NuSMV。符号演算规则“霍尔三元组”:  (|ϕ|)P(|ψ|)表示状态ϕ下做动作P到达下一状态ψ，配合if(state) then rules就是状态迁移系统,决策过程中，状态转移依靠状态最弱前置谓词if(state) then rules。除Hoare论文中的简单语言的规则，其他语言构造的规则也已经被Hoare和很多其他研究者开发完成。包括并发、过程、goto语句，和指针。

那么很自然地，**本次报告**主要关心相关研究进展的以下两点：

- 程序设计语言等效的霍尔自动机和神经网络可解释性的关联---抽象解释

- 有限状态自动机和神经网络结构的关联和可解释性---自动机与神经网络可解释性

两部分的流程都是先大概介绍框架脉络，然后挑选几篇来细读。有关人工智能系统形式化相关补充阅读目录，CCF有两篇对新手挺友好的综述介绍类文章《人工智能系统的形式化发展与趋势》[^人工智能形式化CCF].《形式化方法的研究进展与趋势》[^形式化方法CCF]，此强调，感兴趣者请千万要自行阅读相关引用论文。假如需要形式化相关(Z3之类SMT)的话,大概率需要解SMT-lib[^SMT-LIB]。

Curry-Howard-Lambek correspondence  同构显示推理系统和程序语言之间的相似性。如果决策结构把问题抽象成一个任务和一个问题解映射函数。决策结构初始化神经网络结构代表的函数族就是为找对应问题的解函数映射，又知道文法就是符号版自动机，那么神经网络的可解释性工作就很多方向可以开展。

## 1.2. 抽象解释

抽象解释理论是一种对程序语义进行可靠抽象（或近似）的通用理论[^Abstract Interpretation],该理论为程序分析的设计和构建提供一个通用的框架[^Systematic design]，并从理论上保证所构建的程序分析的 终止性和可靠性（即考虑所有的程序行为）。抽象解释本质上是通过对程序语义进行不同程度的抽象以在分析精度和计算效率之间取得权衡。这种由（面向某类性质的）语义抽象 及其上的操作所构成的数学结构称为抽象域。到目前，已出现数十种面向不同性质的抽象域。其中，代表性的抽象域有区间抽象域、八边形抽象域、多面体抽象域等。实际情况中，抽象域一般包含某些类型的特殊形状，而这些形状又可以被表达成逻辑约束的合集。比如，在欧几里得空间中，最为广泛的抽象域有区间（interval)、环带胞形 (Zonotope)[^zonotope]、多面体（polyhedron)等。开源的抽象域库有 APR0N[^numerical abstract domains]和 ELINA[^ELINA]等。 

[^Abstract Interpretation]:  Cousot, R  Cousot.  Abstract Interpretation ：  a unified lattice mode for static analysis of programs by construction or approximation of fixpoints[ C ] .  POPL 1977 , 1977 ： 238-25
[^Systematic design]:  Cousot, R  Cousot.  Systematic design of program analysis frameworks[ C ] .  In POPL 1979 , 1979 ： 269­282.
[^zonotope ]: Khalil Ghorbal, Eric Goubault, and Sylvie Putot. The zonotope abstract domain taylorl+ [C].CAV2009.
[^numerical abstract domains]: Bertrand Jeannet, Antoine Min. Apron: A library of numerical abstract domains for static analysis[C].CAV2009. pp.661-667.
[^ELINA]: ELINA: ETI Library for Numerical Analysis. http://elina.ethz.ch

如果把一个神经网络看作顺序执行的若干赋值语句组成的程序，则可以使用抽象解释技术分析神经网络。被用于神经网络验证的抽象域包括区间抽象域、zonotope抽象域、多面体抽象域等数值抽象域。神经网络可以看作是一类特殊的程序，输入一般是高维的，激活函数是非线性的， 而且实际应用中神经网络中包含的神经元数量往往非常庞大。因此，对神经网络进行精确推理代价很大，从而需要采用抽象解释对神经网络的具体语义进行抽象，使得在抽象语义上进行推理复杂度更低、效率更高。抽象解释在神经网络验证领域做到效率和精度的权衡，是目前最流行的神经网络验证技术。抽象解释基于严格的理论，保证基于上近似抽象的推理具有可靠性。基于上近似抽象推理得出的性质，在神经网络中一定成立。但是由于抽象中上近似量带来精度损失，不能够保证所有在神经网络中成立的性质都能推理得到。

苏黎世理工学院（ETH) 的 Vechev 领导的研究组最早提出一种基于抽象解释的框架$AI^2$, 来验证神经网络的安全性和鲁棒性[^AI2]，其主要思想是使用一组带条件的仿射函数来建模基于 ReLU 的神经网络，可以刻画神经网络中的全连接、卷积和max-pooling等多种结构，在验证过程中使用区间抽象域、zonotope抽象域及其幂集抽象域来分析这些仿射函数，最后得到输出层变量的取值范围或变量之间的约束关系。文 献 [^zonotope ]完整地介绍$AI^2$并且对20个神经网络进行广泛的评估实验。

[^AI2]:T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri and M. Vechev, "AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation," 2018 IEEE Symposium on Security and Privacy (SP), 2018, pp. 3-18, doi: 10.1109/SP.2018.00058.

结果表明：

1) $AI^2$足够精确以证明有用的软件规范或者性质（例如，稳健性）；

2) $AI^2$可以用来验证最新的稠密神经网络防御的有效性；

3) $AI^2$明显快于基于符号分析的现有工具，后者通常需要数小时才能验证简单的全连接神经网络；

4) $AI^2$可以处理深度卷积网络，这是当时其他基于线性规划和SMT的方法无法企及的。

在这之后，一些针对神经网络验证的抽象域相继被提出。ETH研究组对zonotope抽象域的抽象转换操作进行改进，以更契合神经网络中非线性激活函数的特点，支持 ReLU、tanh、sigmoid激活函数，并基于改进后的zonotope抽象域开发 DeepZ系统[^DeepZ], 以验证神经网络的鲁棒性，并在包含8 万多个神经元的神经网络上开展实验。  DeepZ 不再通过对使用抽象域的交约束和并操作处理激活函数，而是在经典zonotope抽象域的基础上为ReLU、sigmoid和tanh的激活函数增加抽象变换，取得更精确的结果。

[^DeepZ]:Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Piischel, Martin T. Vechev. Fast and effective robustness certification[C]. NeurIPS 2018. pp.10825-10836.

另外，该研究组还面向神经网络验证设计专门的子多面体抽象域[^certifyingNN] ,其主要思想 是为每个神经元节点维护一个值区间、一条符号化上界约束和一条符号化下界约束，并针对仿射转换、ReLU函数、sigmoid函数、tanh函数、max-pooling函数等神经网络中的常见函数设计定制化的抽象转换操作。该抽象域能够证明诸如输入图像被旋转等复杂扰动下神经网络的鲁棒性。基于该抽象域，ETH研究组开发相应的工具[DeepPoly](https://github.com/eth-sri/eran), 并考虑抽象域的可靠浮点实现方法以提高验证的可扩展性和效率，在支持关系型约束和 提高分析精度的同时，也能有效地验证大型网络。

[^certifyingNN]:Singh, G., Gehr, T., Püschel, M., & Vechev, M. (2019). An abstract domain for certifying neural networks. *Proceedings of the ACM on Programming Languages*, *3*(POPL), 1-30.

为提高基于抽象解释的神经网络验证的精度，ETH研究组将基于抽象解释的方法 与更为精确的基于线性规划的方法进行结合[^Boosting]，设计启发式策略来定位哪些神经元在采用抽象解释的上近似分析（如使用zonotope抽象域）之后所得的区间信息仍需要采用基于线性规划的方法来进行进一步精化。基于该方法，ETH研究组实现一个神经网络验证系统RefineZono, 并通过实验表明该方法既能提高基于抽象解释的不完备 (incomplete) 验证方法的精确性，又能够验证一些目前完备验证方法因可扩展性限制不 能验证的鲁棒性质。除在神经网络验证方面，ETH研究组还在结合形式化方法的神经网络训练与查询方面开展研究，提出一种称为可微抽象解释（ differentiable abstract interpretation) 的方法，能够利用抽象解释来训练大规模神经网络，并保证训练出来的神经网络天然满足一些鲁棒性质。该研究组还提出一种面向深度学习的可微逻辑（ Differentiable Logic)[^DI2]，并开发系统[DI2](https://github.com/eth-sri/dl2) , 以支持带逻辑公式约束的神经网络的训练和查询。DL2 支持对模型的输入、输出和内部进行逻辑规约，但不支持量词。使用DI2 , 用户可以通过编写逻辑约束的方式对领域知识进行声明性规约并要求神经网络训练过程中必须遵循， 或者对神经网络模型提出查询，以找到满足给定逻辑约束（如违反鲁棒性）的输入。 DI2 的内部工作原理是将逻辑约束转化为具有良好数学性质的可微损失函数（如将合取 操作转换为损失函数的加法、析取操作转换为损失函数的乘法等），然后使用标准的基于梯度的方法对损失函数进行最小化优化。DI2 在无监督学习、半监督学习、有监督学习 等学习场景下都取得较好效果。

[^Boosting]:Gagandeep Singh, Timon Gehr, Markus Puschel, Martin Vechev. Boosting Robustness Certification of Neural Networks[C]. ICLR 2019.
[^DI2]:Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, Martin Vechev. Dl2: Training and Querying Neural Networks with Logic[C]. ICML2019.

### 1.2.1. Differentiable Abstract Interpretation for Provably Robust Neural Networks

#### 1.2.1.1. 理论

本文一种可扩展的基于抽象解释的用于训练鲁棒神经网络的方法。首先介绍均衡考虑取舍效率和精度并显示的实验证明可用于训练大型神经网络几种abstract transformers，经证实这些方法可抵抗对抗性扰动。

神经网络在诸如面部识别和自动驾驶等关键领域中越来越重要，即使训练有素的神经网络也可能对输入进行错误分类。一种早期的技术来抵御对抗性示例，方法是在训练集中添加具体噪声并将其统计移除，实际上这种防御有时会加剧该问题。

使用抽象解释来近似网络本身代表的非线性函数，利用Abstract Interpretation的经典框架来解决上述挑战，抽象解释是一种用有限表示来近似潜在的无限行为集的通用理论。在过去的40年中，该理论已被广泛用于构建大型自动代码分析器。将展示如何桥接抽象解释和基于梯度的优化，以及如何将这些概念应用于训练更大的网络。具体而言，计算出对抗性多面体抽象域的近似值，并将该近似值用作损失函数的一部分，从而可以在输入空间的整个区域上立即有效地训练网络。这种抽象的损失具有通过诸如梯度下降之类的标准技术进行优化的优势，并且正如所展示的那样，以这种方式训练的网络更加可靠。

本文使用抽象解释来近似神经网络相关研究，已经有工作使用抽象解释来证明神经网络的鲁棒性。Chaudhuri等人还使用黑盒优化技术将其用于小型程序中的常量查找，他们通过连续函数族来近似特定概率域的抽象transformers。

本文主要贡献是：

•一种基于抽象解释的训练神经网络的新方法

•用于zonotope domain的新型抽象transformers，该transformers可并行化，适用于微分和梯度下降。

•完成一个完整实现名为$Diff AI$的[系统](http://safeai.ethz.ch/)，以及对一系列数据集和体系结构的广泛评估。结果表明，$Diff AI$可以提高鲁棒性的可证明性，并可以扩展到大型网络

这里，介绍必要的一般概念和特定的实例。首先定义问题中网络的$\epsilon-robust$鲁棒性，严谨数学定义形式化很复杂，简单理解就是网络$\mathbb{N}_\theta:\mathbb{R}^d \rightarrow \mathbb{R}^k$如果给定扰动$\widetilde{x} = \pi(x),\pi :\mathbb{R}^d \rightarrow \Rho(\mathbb{R}^d)$,网络分类结果的所有class还是相同的。同时给定网络的一个可靠近似的（sound approximation）函数也给定形式化，关键是approximate worst-case adversarial loss$L^A_N(x,y) = \mathop{max}\limits_{\widetilde{z}\in A_N,\pi(x)}L(\widetilde{z},y)$。说白就是用抽象解释的函数定义来近似特定网络，然后对输入进行扰动分析抽象解释函数灵敏度分析类似的操作，来辅助训练可靠robust的网络。

下图一图示抽象解释和神经网络中具体语义卷积全连接层对应映射关系：

![](.\_v_images\Diff1.png)

An abstract domain $D$ is a set equipped with an abstraction function $α:P(\mathbb{R}^p)→ D$and a concretization function $γ:D →P(R^p)$for some $p∈N$.

**Interval  Domain** 间隔算法不是很精确，因为它不保留有关变量值如何关联的信息。而zonotope域旨在保存一些关系。与区间域不同，区间域中的每个误差项都与一个特定的组件相关联，而Zonotope 域则在各个组件之间自由地共享误差项。以此方式，可以以适度的成本来编码一定量的依赖性信息。zonotope域的最重要特征是存在一个仿射函数的抽象转换器（例如完全连接或卷积层的转换函数），并且不会丢失精度。

**Hybrid Zonotope Domain**虽然区带域比间隔域更精确，但其transformer效率较低。该混合zonotope域，最初是一种*扰乱仿射算术*由[Goubault＆Putot（2008年）32)，旨在解决这一问题：它的transformer比间隔更精确，但比zonotope更有效。

首先介绍用于混合带状区域的抽象transformer，特别是在精度与可扩展性之间取得平衡的ReLUtransformer。转换器是“逐点”的：它们可以在GPU上高效地执行，并且可以有益于网络的训练和分析。

共有三种基本类型的抽象transformer，它们：

（i）增加偏差，

（ii）引入新的误差项，从而产生带*m*的混合zonotope

（iii）分别处理偏差和误差系数，并且不引入新的误差项。

那么就可以在抽象解释下的domain中引入对抗训练。过程很复杂，但是就像前面那张图的示例，只要把网络操作算子对应映射过去抽象解释下近似函数的计算就行。

然后经过处理下面的函数分别对应的抽象转换器：

**加法**

**乘法**

**矩阵乘法和卷积**

举个简单的例子：

**ReLU**  is a simple nonlinear activation function：$RELU（x）-max(x,0)$。以这种方式定义的ReLU无法并行化，因为ReLU在每个*组件*中的应用都依赖于先前组件中的应用，时序相关。

然后在我们的领域中引入对抗训练，**近似最坏情况下的对抗损失**

就能进行设计代码和实验。

#### 1.2.1.2. 实验

在称为$Diff AI$中实施本文方法并在一系列数据集和网络体系结构中对其进行广泛的评估。在四个不同的数据集：MNIST，CIFAR10，Fashion MNIST（F-MNIST）和SVHN。还考虑各种大小的网络，如表1所示（SVHN的大小与CIFAR10相同，Fashion MNIST的大小与MNIST相同）。实验证明DIFFAI网络规模更大，比以前的工作的较大先验知识网络与state of art训练有素的网络相比，扰动防御训练稳健性更强。
![Diff2](.\_v_images\Diff11.5.png)
表1.显示我们的网络规模，训练一个历时（平均超过200个历时）所需的时间，以及500个样本的最佳总测试时间，每个域的GPU内存允许的最大批处理大小和网络的结合。测试时间是针对基线训练的网络，而Box训练网络的测试时间是相似的。



![Diff2](.\_v_images\Diff2.png)**表2.** 200个epochs后的时间，测试错误和对抗性边界的结果，L2正则化常数为0.01，PGD迭代为5。

![Diff3](.\_v_images\Diff3.png)

结果分析：

**训练的可扩展性**据所知，就神经元数量和权重而言，在可证明的鲁棒性的背景下分析并捍卫迄今为止考虑的最大网络。如上所示能够在每epochs不到75秒的时间内训练包含124000个神经元和1600万个重量的网络（CIFAR10上的Conv Super），总时间少于5小时。训练过的网络比Kolter＆Wong的工作所考虑的最大网络要大，后者花10个小时来训练一个明显更小的网络，并且没有报告独立的测试速度。通常，Diff AI 5s Box训练甚至比经过5次迭代的PGD训练更快。对于表2中CIFAR 10上的Conv Super Box花费不到4.5个小时来训练，而PGD花费超过8个小时。

**测试的可伸缩性**DIFF AI还可以分析大型网络：对于给定的例子中，它可以在对下验证CIFAR10 Conv Super *2×* IQ -4秒盒和0.1秒 hSwitch。与当前的最新技术相比，这是一个数量级的提速。

**对复杂网络的适用性**还针对所有数据集训练并测试具有剩余连接的网络Skip。MNIST和CIFAR10的可伸缩性结果显示在表1Box训练的对抗性能显示在表2,尽管Skip相当宽，但它只有5个深度，只有一个剩余连接，使用级联而不是加法。

**可证明性**PGD防御往往会略微提高基线精度，与**PGP**攻击相比，那些网络通常不受PGD攻击的攻击。但是，经过盒训练的网络比通过PGD防御的网络更可靠（通过Box或hSwitch）。表2还显示Box训练产生的可证明的稳健网络比基线更可靠，而且经常会损失很少的准确性。

DIFF AI稳定达到4％以下试验误差在MNIST基准卷积网络，如可在表中可见。当使用ConvSuper达到1％测试误差和DIFF AI可以证明一个上限的3.6％的对抗性试验误差，接近下限的由PGD给出2.8％。相比之下，基线训练产生的网络准确性较低，并且对于任何测试示例都无法证明其健壮性。

#### 1.2.1.3. 结论

展示如何应用抽象解释来防御神经网络对抗对抗性扰动，并介绍几种精确地平衡精度与可伸缩性的区域transformer。

结果表明，该训练方法可以扩展到比以前的工作更大的网络，并且所证明的网络比经过最新防御训练的网络更可靠。

### 1.2.2. DL2: Training and Querying Neural Networks with Logic

随着神经网络在广泛的应用领域中的成功，一个重要的新挑战是为专家创建更好，更灵活的机制来与网络交互。例如，人们可能希望网络捕获某些不容易获得的背景知识作为标记的训练数据，或者希望网络5的决策如何在特定领域特定的输入下发生变化。从根本上讲，要清晰地捕获此类交互，就需要某种形式的逻辑推理，以及将这种推理与神经网络的训练和决策程序相结合的一种方式。的确，最近的工作已经开始研究神经推理和逻辑推理的新颖组合。但是，如稍后所述，尽管这些方法很有希望，但它们缺乏通用性，无法捕获重要的用例。

本文提出DL2，这是一种用于训练和查询具有逻辑约束的神经网络的系统。

使用DL2，可以声明性地指定要在训练期间实施的领域知识约束，也可以对模型进行查询以找到满足一组约束的输入。DL2通过将逻辑约束转换为具有所需数学属性的损失函数来工作。然后，使用基于标准梯度的方法将损耗降至最低。通过在无监督，半监督和有监督的环境中训练带有有趣约束的网络来评估DL2。

实验评估表明，DL2比结合逻辑和神经网络的现有方法更具表现力，并且其损失函数更适合优化。（具有可区分逻辑的深度学习的缩写），可用于：

（i）以声明的方式查询网络中符合逻辑约束的输入，以及

（ii）训练网络以满足逻辑规范。约束语言可以使用否定，合取和析取来表示神经网络的输入，神经元和输出的比较丰富的组合。

得益于它的表现力，DL2使用户可以轻松地在训练过程中合并领域知识，并可以使用查询与网络进行交互，以查询其决策。

从技术上讲，DL2将逻辑约束转换为具有两个关键属性的非负损失函数：

（i）如果满足约束，则损失为零；

（ii）几乎在任何地方都可以区分损失。

这些属性结合在一起，使能够通过使用现成的优化器将损失降到最低来训练和查询约束。

使用DL2进行训练为使优化易于处理，提取捕获某些种类凸集的输入约束，并将其用作优化约束，而不是将它们包括在优化目标中。然后，使用投影梯度下降（PGD）进行优化，该方法已被证明在具有鲁棒性约束的训练中是成功的。

DL2的表达能力以及PGD的易处理性优化使能够进行新的有趣约束训练。

**本文主要贡献**是：

•一种用于训练和查询具有逻辑约束的神经网络的方法，该方法可将约束转换为具有所需属性的损失函数。

•一种训练程序，该程序提取捕获凸集的输入的约束并将其作为PGD约束包括在内，从而使优化变得容易。

•用于查询神经网络输入，输出和内部神经元的声明性语言。查询被编译为损失并使用L-BFGS-B进行优化。

•广泛的评估表明DL2对于查询和训练神经网络有效。在其他实验结果中，显示DL2成功训练受训练集外部输入约束的网络。

#### 1.2.2.1. 理论

介绍约束语言，并展示如何将逻辑约束转换为（非负）损失。关键是： how to translate logical constraints（constraint language ） into a (non-negative) loss：

$L(φ) = 0\ if\ and\ only\ if\ φ\ is\ satisfied$,这就是逻辑约束转变为损失函数的关键。逻辑约束语言由术语之间比较的布尔组合组成。

最相关的工作，并将它们与[DL2](https://github.com/eth-sri/dl2)进行对比：

概率软逻辑（PSL）将逻辑约束转换为连续的几乎在[0,1]上的任何可微处的损失函数。但是，由于梯度可能为零，因此使用这种损失用梯度方法找到满意的分配可能是徒劳的。Evans还分析布尔组合的不同标准选择（t-范数）编码，并发现最适合梯度下降的乘法。Hu等在PSL的基础上提出一种teacher-student框架，该框架将规则提炼到培训阶段。这个想法是用封闭形式的解决方案将规则满足表达为一个凸问题。但是，这种表述仅限于输入和输出类的规则，不能表达约束输出激活数值的规则

相反，DL2可以表示这样的约束，例如$p_i> p_2$ ，这要求类别1的输出激活大于类别2的输出激活。而且，闭式解的凸性和存在性源于线性网络输出中的规则，这意味着非线性约束（例如，Lipschitz条件，可以用DL2表示）从根本上超出此方法的范围。Xu等人工作同样受到输出约束的限制概率，并且对于许多变量中的较大约束而言都是棘手的。与DL2不同，这两个作品都不支持回归任务的约束。

XSAT 降低浮点公式对数值优化的可满足性。为此，XSAT还可以将逻辑约束转换为数值损失。但是，其原子约束转换为基于浮点表示的离散的，无处可微的损失，这是基于梯度的优化所无法实现的。

Bach等引入硬线性约束的软约束版本。DL2对这种类型的约束使用相同的方法。但是，DL2更通用 ：

它允许对域R的变量进行非线性约束的任意布尔组合，而Bach等人仅考虑线性约束对具有域[0,1]的变量的合取。

![DL2-1](.\_v_images\DL2-1.png)

训练具有**约束条件的神经网络**的方法。

![DL2-2](.\_v_images\DL2-2.png)



在DL2的基础上，设计一种用于查询神经网络的声明性语言。有趣的是，先前工作研究的许多问题现在都可以表述为DL2查询：负责预测的神经元，区分网络的输入和对抗性示例。语言支持更多查询功能，附加功能，提高便利性。例如，用户可以指定和操纵张量。在查询中，用逗号（，）表示连词；在forbox约束中，使用类在给定的输入上返回网络的输出标签，即具有最大概率的类。

**解决查询**与培训一样，将约束编译为损失。损耗被最小化，直到达到零或超时为止。与训练不同，使用L-BFGS-B执行优化，它比标准梯度下降算法慢，但更复杂。可以负担得起，因为优化的损失函数不依赖于整批训练样本。

**查询优化**在这里，讨论如何针对L-BFGS-B优化损耗编译。虽然翻译是针对任意大的约束定义的，但总的来说，d在许多方面优化损失。因此，通过从表达式中提取框约束来使损失更小。然后根据其余约束条件来计算损失。提取的盒约束被传递到L-BFGS-B解算器，该解算器然后用于查找损失的最小值。这使能够从损失中排除^的主要部分，从而使损失易于优化。

#### 1.2.2.2. 实验

提出一个全面的实验评估，该评估表明DL2在查询和训练具有逻辑约束的神经网络方面的有效性。

系统在PyTorch中实现并在Nvidia GTX 1080 Ti和4.20 GHz的Intel Core i7-7700K上进行评估。在MNIST，FASHION ，CIFAR-10和CIFAR-100四个数据集的各种任务（有监督，半监督和无监督学习）上评估DL2 。在所有实验中，在损失中添加一个交叉熵项，以优化预测精度。对于每个实验，都描述其他逻辑约束。

![DL2-3](.\_v_images\DL2-3.png)

**半监督学习**对于半监督学习，专注于CIFAR-100数据集，并将训练集分为比例为20/60/20的标记集，未标记集和验证集。本着实验精神考虑约束条件，该约束条件要求类别组的概率具有非常高的概率或非常低的概率。一组由相似类型的班级组成（例如，婴儿，男孩，女孩，男人*和*女人*班级是*人组的一部分），组5的概率是其班级的概率之和。

**无监督的学习**接下来，考虑在无监督的情况下进行回归任务。任务是训练MLP（多层感知器），以预测从源节点到未加权图中*G* = （V，E）的每个节点的最小距离 。

**监督学习**认为**监督学习有**两种类型的约束：

*全局约束（*具有（可能不止一个）普遍量化的变量z）和*训练集约束*，其中所有变量均指训练集中的样本（无量化变量z）。请注意，一般而言，没有任何先前的工作适用于*全局约束*。此外，由于其编码的局限性先前的工作无法处理实验中考虑的复杂*训练集约束*（例如，输出激活之间的比较）。将随机样本写为*x*和*y，* 用相应的标签表示训练集中的输入。

**用DL2查询**

接下来，对具有约束的查询任务评估DL2。

考虑五个图像数据集，每个数据集具有不同的完善的神经网络体系结构。对于每一个，至少考虑两个分类器。对于某些生成器和鉴别器（使用Goodfellow等的GAN进行训练）（附录B表6提供有关这些网络的详细信息）。基准测试包含18个模板查询（在附录B中提供），这些查询使用不同的网络，类和图像实例化。

![DL2-4](.\_v_images\DL2-4.png)

图3（右）显示模板查询，该查询尝试搜索生成器G的输入，以使其输出由分类器Ni和n2分为两个不同的类ci和c2。对于每个数据集，将每个查询模板实例化为10个查询。查询将一直运行，直到找到正确的解决方案或达到2分钟的超时为止。

图3（左）显示每个查询模板和数据集的平均运行时间。结果表明，系统通常会找到解决方案，并且通常会在40秒内完成。尚不清楚实际上未解决的查询是否确实具有解决方案。

得出结论，成功的执行相对迅速地终止，并且DL2可以很好地扩展到多个大型网络（例如，对于ImageNet）。附录B和附录C提供更多详细信息和实验。

#### 1.2.2.3. 结论

提出DL2，这是一种用于训练和查询具有逻辑约束的神经网络的方法和系统。DL2支持表达逻辑约束，并将转换规则提供到几乎无处不在的损失，对于满足约束的那些输入，该损失恰好为零。为使训练易于处理，处理输入约束，这些约束通过PGD捕获凸集。还引入一种声明性语言，用于利用从逻辑到损失的转换来查询网络。实验结果表明，DL2可以有效地用于训练和查询带有附加约束的神经网络。


## 1.3. 神经网络与有限状态自动机

虽然神经网络在很多应用领域中都展示它具有良好的性能（效率高、准确率高 等），但是，神经网络的一个主要缺点是神经网络没能对它内在的推理机制提供一个很好的解释或者说明，这很大程度上限制它的应用和推广，尤其是在安全攸关方面的应用。 

大多数研究者认为主要的原因是目前缺乏相应的技术理解神经网络的决策过程。此外， 对神经网络所学习到的知识及神经网络系统本身如何进行形式化建模也依然处于探索阶段。为此，研究者们提出各种技术来对提取神经网络系统进行的知识萃取或形式化建模，包括有限自动机、规则集合、决策树和程序等。

由于有限自动机与神经网络（尤其是RNN) 之间存在着比较大的联系或者相似之处[^structure in time]，大多数研究工作采用有限自动机来表示神经网络的知识萃取或模型。在20世纪 90年代早期，研究者们就开始尝试从处理序列数据的RNN中提取自动机。一般来说，从 RNN中提取自动机的技术可以归纳为以下四个步骤[^Rule ExtractionRNN]:

1 )  量化：神经网络（比如RNN) 连续状态空间的量化。

2 )   状态生成：根据输入生成可能状态和输出以及其分类（如果需要的话）。

3 )   规则构造：基于观察到的状态迁移构造规则。

4 )   规则最小化：对规则集合进行合并优化。

早期的自动机提取技术主要采用**层次聚类**分析（ Hierarchical cluster analysis) 来分析神经网络的连续状态空间[^Hierarchical1][^Hierarchical2][^Hierarchical3][^Hierarchical4][^Hierarchical5][^Hierarchical6]，但是这种方法可能不易找出类与类之间的（临时）关系。

随后，一些研究者提出将状态空间均匀地抽象成n个超立方体（即宏观状态），然后采用深度优先搜索策略来**搜索**神经网络的可能状态集。然而，这种技术最大的问题在于聚类数目会随着状态节点数的增长而指数增长。有些研究者基于向量的量化来对状态空间进行分类。曾等人提出基于k-means的宏观状态聚类方法。但是，为支持合适的状态聚类，这种方法需要在训练的时候引入一些 bias。为此，Alquezai和Sanfeliu采用早期的层次聚类分析，并结合前序树对状态空间进行剪枝。

[^structure in time]:Jeffrey LElman. Finding structure in time[C]. Cognitive Science. pp.14,179-211,1990.
[^Rule ExtractionRNN]:Jacobsson, Henrik. Rule Extraction from Recurrent Neural Networks: ATaxonomy and Review. Neural Computation 2005[J]. pp.17(6):1223-1263.
[^Hierarchical1]:Axel Cleeremans, David Servan-Schreiber, James L. McClelland. Finite slate automala and simple recurrent networks[C]. Neural Computalion 1989. pp.1,372-381.
[^Hierarchical2]:Servan-Schreiber,D Cleeremans,A & McClelland J L. Learning sequential structure in simple recurrent networks[C].D.S. Tourelzky(Ed.), Advances in neural information processing syslems,1(pp.643-652). San Mateo, CA: Morgan Kaufmann,1989.
[^Hierarchical3]:David Servan-Schreiber, Axel Cleeremans, James L. McClelland. Graded stale ma-chines: The representation of temporal contingencies in simple recurrent net-works[C]. Machine Learning, pp.7,161-193.1991.
[^Hierarchical4]:Giles,C1, Miller,C B, Chen,D, Chen,H H,& Sun,G Z. Learning and extracting finite slale aulomala with second-order recurrent neural networks [C]. Neural Compulation. pp.4(3),393-405,1992.
[^Hierarchical5]:Giles, CL, Chen,D, Miller,C, Chen,H, Sun,G,& Lee,Y. Second-order recurrent neural networks for grammatical inference[C]. Proceedings of Internalional Joint Conference on Neural Networks. pp.273-281. Piscalaway, NJ: IEEE,1991
[^Hierarchical6]: Chrislian W Omlin,C Lee Giles. Extraction of rules from discrele-time recurrent neural networks[J].Neural Networks. pp.9(1),41-51,1996.

除对状态空间进行搜索，有些研究者提出基于**采样**的方式来提取自动机。 Watrous和Kuhn[^ Induction of finite-state]提出第一个基于采样的方法，该方法在处理状态空间的同时进行采样，并动态地更新每个宏观状态的区间。Man0li0S和Fanelli[^First orderRNN]使用一个简单的向量量化器，且对给定的测试集进行状态空间采样，但不能保证该过程的终止性。类似的，Tmo 和Sajda[^mealy automata]提出的方法也是对测试集的状态空间进行采样，不同的是他们的方法采用星拓扑自组织映射（self-organizing map) [^Self-organizing]来量化状态空间。但是，这些基于采样的方法 可能存在着宏观状态的不一致性（即不确定性）问题，从而导致提取失败。

为解决不一致性问题，Schellhammei等人[^Knowledge extraction andRNN]引入迁移频率概念且忽视那些最小频率的不一致性迁移。解决不一致性问题的另外一个方案是概率自动机[^probabilistic automata]。Tino和Vojtek[^Extracting stochastic machines]提出一个 从RNN提取概率自动机的方法。该方法采用文献[103]的自组织映射来量化空间，同 时结合域驱动（根据输入观察输出）和自驱动（上次的输出作为下次的输入）来生成状态空间。随后，Tino等人[^Extracting finite-state representations fromRNN][^Understanding state space]提出一个改进方法，将其中自组织映射改为动态细胞结构（ dynamic cell structure)[^Dynamic cell structure learns]。然而，使用概率自动机的弊端是难以找到所提取概率自动机与网络之间的联系。最近，Rabusseau等人[Connecting Weighted Automata]提出基于光谱学习（ Spectral Learning) 的权重自动机提取方法，其中权重自动机是概率自动机的一种一般化。

[^Induction of finite-state]:Watrous, RL,& Kuhn, GM. Induction of finite-state automata using second-order recurrent networks. J.E. Moody,S.J. Hanson,&R.P. Lippmann(Eds.), Advances in neural information processing systems,4(pp.309-317)[C]. San Maleo, CA: Morgan Kaufmann,1992.

[^First orderRNN]: Manolios,P, Fanelli,R. First order recurrent neural networks and deterministic finite stale automata[C]. Neural Computalion. pp.6(6),1155-1173,1994.
[^mealy automata]: Tino,P,& Saajda,J. Learning and extracting initial mealy automata with a modular neural network model[J]. Neural Computation. pp.7(4),822-844,1995.

[^Self-organizing]: Kohonen,T. Self-organizing maps[M]. Berlin: Springer,1995.
[^Knowledge extraction andRNN]: Ingo Schell hammer, Joachim Diederich, Michael Towsey, Claudia Brugman. Knowledge extraction and recurrent neural networks: An analysis of an Elman nelwork trained on a natural language learning task[C]. CoNLL1998. pp.73-78.
[^probabilistic automata]: Pax,A.. Introduction to probabilistic automata[C]. Orlando, FL: Academic Press,1971.
[^Extracting stochastic machines]: Peler Tino,V Vojtek. Extracting stochastic machines from recurrent neural networks trained on complex symbolic sequences[C]. Neural Network World. pp.8(5),517-530,1998.
[^Extracting finite-state representations fromRNN]: Peter Tiio, Miroslav Koleles. Extracting finite-state representations from recurrent neural networks trained on chaotic symbolic sequences[J]. IEEE Transactions on Neural Networks. pp.10(2),284-302,1999.

[^Understanding state space]:Peter Tino, Georg Dorffner, Christian Schittenkopf. Understanding state space organization in recurrent neural networks with iterative function systems dynamics.S. Wermter & R. Sun(Eds.), Hybrid neural symbolic integration. pp.256-270[M]. Berlin: Springer-Verlag,2000.

[^Dynamic cell structure learns]:Jorg Bruske, Gerald Sommer. Dynamic cell structure learns perfectly topology preserving map[C].Neural Computation. pp.7,845-865,1995.
[^Connecting Weighted Automata]:Guillaume Rabusseau, Tianyu Li, Doina Precup. Connecting Weighted Automata and Recurrent Neural Networks through Spectral Learning[C]. CoRR, abs/1807.01406.2018.

上述所提的方法几乎都属于**白盒方法**，就是说，这些方法需要分析神经网络的结构及其内部状态。

一些研究者们提出一些不需要解神经网络的结构及其内部状态的**黑盒方法**。

Vahed和Omlin[^symbolic machine learning][^extracting symbolic knowledge]关注限定长度的输入及其输出，提出基于机器学习的自动机提取方法，但其前序树的复杂度比较高。WeiSS等人[^queries and counterexamples]采用 L\*算法[^counter examples]和抽象技术从RNN中学习出有限自动机。该方法只能够有效地应用到字母表比较小的正则语言。 Mayi和YOvine[^Regular Inference]提出基于有限界L \*算法的自动机提取算法，且该算法能够保证所提 取的自动机符合s-近似正确。最近，Ayache等人[^weighted automata]提出一种从应用于序列数据的黑盒系统中提取权重自动机的学习方法。基于L \*算法的扩展算法，Okudono等人[^weightedautomataextraction]提出 权重自动机的学习算法。不同于Ayache等人的方法，Okudono等人的方法充分利用内部状态来完成等价查询。

[^symbolic machine learning]: Vahed,A,& Omlin,C W. Rule extraction from recurrent neural networks using a symbolic machine learning algorithm(Tech. Rep. No. US-CS-TR-4)[C]. Stel-lenbosch, South Africa, University of Stellenbosch,1999.
[^extracting symbolic knowledge]: Vahed,A,& Omlin, CW.A machine learning method for extracting symbolic knowledge from recurrent neural networks[C]. Neural Computation. pp.16,59-71,2004.
[^queries and counterexamples]: Gail Weiss, Yoav Goldberg, Eran Yahav. Extracting automata from recurrent neural networks using queries and counterexamples[C]. ICML2018. pp.5244-5253.
[^counter examples]: Angluin,D. Learning regular sets from queries and counter examples[C]. Inf. Compul.. pp.75(2):87-106,1987.
[^Regular Inference ]: Franz Mayr, Sergio Yovine. Regular Inference on Artificial Neural Networks[C]. Machine Learning and Knowledge Extraction. pp.350-369.2018.

[^weighted automata]: Stephane Ayache, Remi Eyraud, and Noe Goudian. Explaining black boxes on sequential data using weighted automata[C]. ICGI2018. pp.81-103.
[^weightedautomataextraction]: Takamasa Okudono, Masaki Waga, Taro Sekiyama, Ichiro Hasuo. Weighted Automata Extraction from Recurrent Neural Nelworks via Regression on Stale Spaces[C]. CoRR, abs/1904.02931.2019.
随着人工智能（特别是深度学习）的发展，一些研究者关注结构比较复杂的新型网络。王等人将基于k-means方法应用到三个新型网络中，即LSTM (long-short­term-memory networks) , GRU ( gated- recurrent- unit networks) 和 MI-RNN ( multiplicative integration recurrent neuron networks)。Koul等人[^Policy Networks]关注应用于强化学习和模仿学习的 RNN策略网络，并引人量化瓶颈插人技术（ quantized bottle neck in sertion) 来从RNN策 略网络中提取摩尔机网络。Ikram等人[^learning of sequences]提出基于k-means 从LSTM中提取自动机的方法。Lu和liu[^cyclic switchings]提出一种基于注意力抽象的方法，从DOB-net中提取有限自动机，即关键摩尔机器网络（ Key Moore Machine Network) , 以捕获其控制转换机制。

[^Policy Networks]: Anurag Koul, Alan Fern, Sam Greydanus. Learning Finite State Representations of Recurrent Policy Networks[ C]. ICLR 2019.

[^learning of sequences]: Ikram Chraibi Kaadoud, Nicolas P Rougier, Frederic Alexandre. Knowledge extraction from the learning of sequences in a long short term memory(LSTM) architecture[C]. CoRR abs/1912.03126.2019.
[^cyclic switchings]: Wenjie Lu, Dikai Liu.A2: Extracting cyclic switchings from DOB-nets for rejecting excessive disturbances[C]. Neural computing. pp.400:161-172,2020.

近两年来，一些研究者们试图从形式语言的角度来理解RNN, 比如比较RNN与有限自动机的状态或计算能力，和从RNN中提取形式语言。William[^Sequential NN]通过将神经网络关联 到有限自动机来解释神经网络的计算能力。Joshua等人[^Representing Formal Languages]分析和比较RNN在识别正则 语言时的内部状态与接受该语言的最小有限自动机（MDFA) 的状态的关联关系，发现 RNN的内部状态可以映射到MDFA的超状态。Wang等人[^Deterministic Finite Automata]试图将具有不同阶隐藏交互 的RNN与不同复杂度的正则文法关联起来。Christian等人[^InterpretationRNN]从精确性和可解释性分析 用正则语言训练的简单RNN的行为，他们发现适当的调整参数能使网络同时具备较强 的泛化能力和可解释为有限状态自动机。Reda等人[^Distance and Equivalence]从理论上研究从RNN中提取有 限状态自动机的一些性质。Bishwamittra和Daniel[^Formal Language Approach]结合近似可能正确（ Probably ApproximatelyCorrect) 和约束求解，提出从RNN中提取线性时序逻辑（LTL) 的方法， 以解释RNN的决策过程。

[^Sequential NN]: William Merrill. Sequential Neural Networks as Automata[C]. CoRR abs/1906.01615.2019.
[^Representing Formal Languages]: JoshuaJMichalenko, Ameesh Shah, Abhinav Verma, Richard G Baraniuk, Swarat Chaudhuri, Ankit B Patel. Representing Formal Languages:A Comparison Between Finite Automata and Recurrent Neural Networks[C]. ICLR 2019.
[^Deterministic Finite Automata]: Qinglong Wang, Kaixuan Zhang, Xue Liu,C. Lee Giles. Connecting First and Second Order Recurrent Networks with Deterministic Finite Automata[ C]. CoRR abs/1911.04644.2019.

[^InterpretationRNN]: Christian Oliva, Luis F Lago-Fernandez. On the Interpretation of Recurrent Neural Networks as Finite Stale Machines[J]. ICANN(1)2019. pp.312-323.
[^Distance and Equivalence]:Reda Marouk, Colin de la Higuera. Distance and Equivalence between Finite State Machines and Recurrent Neural Networks: Computational results[ C]. CoRR abs/2004.00478.2020.

[^Formal Language Approach ]: Bishwamittra (Ghosh, Daniel Neider.A Formal Language Approach to Explaining RNNs[C]. CoRR abs/2006.07292.2020.


此外，还有一些研究工作从网络中提取符号化规则[^refined rule][^acquisition][^pruning and hidden-unit][^FERNN][^regularizalion]、决策树[^tree-structured][^ANN-DT][^Tree Extraction]和逻辑程序[^parallel computational model][^reduced logic programs][^adversarial examples][^Universal adversarial perturbations]等。

[^refined rule]: Geoffrey G. Towell, Jude William Shavlik. The extraction of refined rules from knowledge-based neural networks[C]. Machine Learning. pp.13,71-101,1993.
[^acquisition]: Sabrina Seslito, Tharam S. Dillon. Knowledge acquisition of conjunctive rules using multilayered neural networks, International Jour-nal of Intelligent Systems 8[C]. pp.779-805,1993.
[^pruning and hidden-unit]: Rudy Setiono. Extracting rules from neural networks by pruning and hidden-unit splitting, Neural Compulalion 9[C]. pp.205-225,1997.
[^FERNN]: Rudy Setiono, Wee Kheng Leow. FERNN: An algorithm for fast extraction of rules from neural nelworks, Applied Intelligence[C]. pp.12,15-25,2000.
[^regularizalion]: Masumi lshikawa. Rule extraction by successive regularizalion. Neural Networks[C]. pp.13,1171-1183,2000.
[^tree-structured]: Mark W Craven, Jude W. Shavlik. Extracting tree-structured representations of trained networks. NIPS1996[C]. pp.24-30.
[^ANN-DT]: Gregor PJ Schmitz, Chris Aldrich, FS Gouws. ANN-DT: An algorithm for extraction of decision trees from artificial neural networks[C]. IEEE Press,1999.
[^Tree Extraction]: Bondarenko Andrey, Aleksejeva Ludmila, Jumute Vilen, et al. Classification Tree Extraction from Trained Artificial Neural Networks. Procedia Computer Science[C]. pp.104:556-563,2017.

[^DistillingNN]: Nicholas Frosst, Geoffrey E. Hinton. Distilling a neural network into a soft decision tree[J]. arXiv preprint arXiv:1711.09784,2017.
[^Tree regularization]: Mike Wu, Michael C Hughes, Sonali Parbhoo, Maurizio Zai, Volker Roth, and Finale Doshi-Velez.Beyond sparsity: Tree regularization of deep models for interprelability[C]. AAAl 2018.
[^Verifying BNN]: Andy Shih, Adnan Darwiche, and Arthur Choi. Verifying Binarized Neural Networks by Angluin-Style Learning[C]. SAT2019.

[^parallel computational model]: Steffen Holldobler, Yvonne Kalinke. Towards a new massively parallel computational model for logic programming[C]. ECCA11994. pp68-77.
[^reduced logic programs]: Jens Lehmann, Sebastian Bader, Pascal Hitzler. Extracting reduced logic programs from artificial neural networks. Applied intelligence[C]. pp.32, no.3:249-266,2010.
[^adversarial examples]: lanJ Goodfellow, Jonathon Shlens, Christian Szegedy. Explaining and harnessing adversarial examples[C]. CoRR, abs/1412.6572.
[^Universal adversarial perturbations]: Seyed-Mohsen Moosavi-Dexfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard, Universal adversarial perturbations[C]. In: CVPR, pp.86-94,2017.

### 1.3.1. Recurrent neural networks and finite automata

#### 1.3.1.1. 理论

递归神经网络能够近似相当任意的动态系统，并且可用于自适应控制和信号处理应用中，它们也构成强大的计算模型。在语音处理应用和语言归纳中，使用递归网络模型作为识别模型，并通过某些成本标准的梯度下降优化（所谓的“反向传播”技术）将其拟合到实验数据中。

自动机或顺序机器是随时间变化，对外部刺激做出反应并进而通过自身动作影响其环境的设备。在计算机科学和逻辑中，*自动机理论*处理该概念的各种形式化形式。在这种形式上，神经网络构成（非常）特殊类型的自动机。因此，自然而然地，通过将神经网络与更抽象的自动机一般模型进行比较，分析神经网络的信息处理和计算能力.本文使用没有初始状态且有顺序输出的有限自动机的一般定义(五元组$f,u,y,f,h$)。

尽管自动机的数学形式化是在数字计算机出现之前进行的，但将计算机视为自动机的范例对于解释基本原理还是很有用的。至少从McCulloch和Pitts（1943）的工作开始就知道，由阈值神经元组成的有限大小的递归网络可以模拟有限自动机。出于在学习和适应连续型网络中的成功应用以及模拟计算的生物学适应性的推动，连续型网络的计算能力变得越来越受关注。Siegelmann和Sontag证明，由一种特定类型的模拟神经元组成的网络可以计算分段线性函数，该网络可以模拟有限自动机（Siegelmann 1993）（甚至可以是各种无限自动机，例如Turing机器和super-Turing模型）。Kilian和Siegelmann（1993）证明具有S型神经元的网络可以模拟有限自动机（和图灵机）。

先前关于递归神经网络的一些工作集中在无限大小的网络上。由于每个神经元本身都是处理器，因此与模型（由有限数量的神经元组成）相比，这种无穷次幂的模型对于计算能力的研究就没有那么有趣。

但是，以前的工作涉及有限网络的可计算性。McCulloch and Pitts的经典结果表明，如何通过阈值网络实现逻辑门，并因此如何通过此类网络模拟有限自动机。

另一个相关的结果是由于Pollack提出的，他称某个特定的递归网络模型是图灵通用的 （ Neuring machine）是Turing通用的。Pollack中的模型由有限数量的两个不同的神经元组成。机器是高阶的，也就是说，激活是用乘法结合的，而不是线性的结合，Hartley和Szu也发现类似的结果。当权重是一般实数时，网络证明具有超级图灵功能。但是，它对资源限制很敏感，因此不是重言式。Kilian and Siegelmann构建一个图灵通用的S型神经元网络，尽管它的速度比Siegelmann-Sontag网络要慢。他们将结果推广到其他类似S形的网。

本文纯理论数学推导证明这个有趣的结论：

![RNN](.\_v_images\RNN.png)

<img src=".\_v_images\RNN2.png" alt="RNN2" style="zoom:100%;" />

> 对于任何激活函数*q* ，其满足特性（★），存在常数$w_0,w_1,wb_i,c_i \in \mathbb{R},i = 1,2,3$有函数$f(y)= w_0+ \sum^3_{i=1}w_i * Q(b_y+c_i)$满足$f(-1) = 0 = f(0) $且$f(1) = 1$
>
> 每个有限自动机可以由具有满足特性（★）的任何激活函数的神经网络模拟。

本文研究这样的有限大小的网络：

这些网络由同步演进的处理器的互连组成。每个处理器通过将激活函数应用于所有单元先前状态的线性组合来更新其状态。本文证明：可以将存在左，右极限并且不同的**任何函数**应用于神经元，以产生至少在计算上与有限自动机一样强大的网络。


#### 1.3.1.2. 结论

本文得出的结论是，如果这是所需的能力，则可以根据特定应用的可用硬件或学习软件来选择任何上述神经元。

本文证明**任何类型的递归神经元网络可以模拟一个有限的自动机**。因此，可以仅根据成本，可用性和特定应用程序的可渗透性来选择**任何**这样的网络来实施。

### 1.3.2. Representing formal languages : a comparison between finite automata and recurrent neural networks

本文研究递归神经网络（RNN）在学习识别常规形式语言时使用的内部表示形式。具体地，在常规语言的正例和负例上训练RNN，并询问是否存在将该RNN的状态映射到该语言的最小确定性有限自动机（MDFA）的状态的简单解码功能。

实验表明，这样的解码功能的确存在，而且它映射RNN的状态不MDFA状态，但到的状态抽象通过将一小组MDFA状态聚类为“超级状态”而获得。定性分析表明，抽象通常具有简单的解释。

总体而言，结果表明RNN所使用的内部表示与有限自动机之间存在很强的结构关系，并解释RNN识别形式语法结构的众所周知的能力。

长期以来，众所周知，RNN在识别文本模式方面表现出色。在探索RNN的表达能力方面已经进行广泛的工作。例如，有限的RNN已被证明能够模拟通用图灵机。Funahashi and Nakamura表明，RNN的隐藏状态可以近似表示相同或更少维复杂性的动力学系统。在特别相似的工作中，Rabusseau等人表明具有线性激活函数的二阶RNN在表达上等同于加权有限自动机。

最近的工作还通过多种方法探索RNN内部与DFA之间的关系。尽管已经有过多次尝试让RNN根据DFA生成的输入语言来学习DFA结构并压低自动机，大多数工作都集中在**提取**来自已学习RNN的隐藏状态的DFA。在该领域的早期工作证明，可以从学习的RNN中提取常规语法的语法规则。

其他研究试图直接从RNN的内部空间中提取DFA结构，通常是通过聚类来自输入刺激的隐藏状态激活，并指出在特定的新输入刺激下从一种状态到另一种状态的转变。聚类是通过一系列方法完成的，例如K最近邻和基于密度的应用噪声空间聚类。另一项提取工Ayache等人使用频谱算法技术从RNN中提取加权自动机。最近（2018），Weiss等通过使用L *查询学习算法在DFA提取中获得最先进的准确性。

本文工作与这些努力的不同之处在于，将RNN直接与**真实的最小DFA**相关联，而不是从RNN的状态空间中提取机器。

最接近的相关工作是蒂诺（Tino）等人。它试图将RNN状态与DFA状态相关联。但是，Tino等人的RNN完全模仿DFA；同样，该研究是在几种特定的常规语言的背景下进行的，这些语言被自动机识别为2-3个状态。相反，工作不需要RNN和DFA之间的确切行为对应：允许*抽象*DFA状态*，*从而导致信息丢失。同样，在方法中，从RNN状态到FA状态的映射可以是近似的，并且对映射的准确性进行定量评估。表明，这使能够在RNN和DFA之间建立联系，存在一类广泛的常规语言，这些语言通常需要比Tino等人研究的自动机大得多的自动机（最多14个state）。

#### 1.3.2.1. 理论

递归神经网络（RNN）似乎“过分”有效地在嘈杂的现实世界序列中对模式进行建模。尤其是，它们似乎可以有效地识别序列中的语法结构，正如它们产生结构化数据（例如源代码C ++/LaTeX（看过有论文把latex证明是图灵完备的）等的计算能力，其实这个就是柯里霍华德同构，前文提到的decision structure）所证明的那样，语法错误很少。RNN识别形式语言（具有严格定义的语法结构的字符串集）的能力研究较少。此外，对RNN*如何*识别严格的结构几乎没有系统的解。

本文旨在通过与形式语言（即有限自动机和常规语言）的基本概念进行比较，来解释这种RNN的内部算法。在本文中，通过将经过训练的RNN与解决相同语言识别任务的有限自动机进行比较，提出一种新的理解受训RNN表示语法结构的方式。

问：经过培训可以识别形式语言的RNN的内部知识表示形式是否可以轻松地映射到传统上用于定义这些形式语言的自动机理论模型的状态？

具体来说，针对*普通语言* 或*有限自动机*（FA）接受的形式语言的类别来调查此问题。

在实验中，RNN在给定形式语言随机生成的字符串的正例和负例的数据集上进练。接下来，询问是否存在解码功能： 一种同构关系，该同构关系将训练后的RNN的隐藏状态映射到规范FA（有穷自动机）的状态。

由于存在无限多种接受相同语言的FA，因此专注于*最小确定性有限自动机*（MDFA）---具有尽可能少的状态数的确定性有限自动机（DFA）-可以完美识别该语言。



![Compare-1](.\_v_images\Compare-1.png)





![Compare-2](.\_v_images\Compare-2.png)

**抽象**非确定性有限自动机（NFA）类似于DFA，不同之处在于确定的过渡函数S现在是一个非确定性转移关系。

**从RNN解码DFA状态。**受Astrand提的计算神经科学方法的启发，可以定义一个*解码函数*或*解码器*

**解码抽象状态**

![decode1](D:\tridu33\postgraduate\深度学习前沿\论文阅读报告\_v_images\decode1.png)

**解码抽象状态转换** 定义一个精度度量，该度量考虑基础函数$f$保留过渡的良好程度。

![decodetran](D:\tridu33\postgraduate\深度学习前沿\论文阅读报告\_v_images\decodetran.png)

直观地，对于给定的解码函数 f 和NFA非确定性状态自动机$A_j$，需要检查**a上的RNN转换**是否映射到**a上MDFA转换**的抽象。

目标是通过实验检验从$R_j$到$A_J$存在高精度，低粗糙度解码器的假设。我们旨在回答与过渡精度有关的4个基本问题：

A_j和Rj的关系：

（1）我们如何选择适当的抽象解码函数f？

（2）需要什么抽象函数a？

（3）我们可以验证存在较低的粗糙度*a*和较高的精度f吗？

（4）如何在*a*和f的上下文中更好地理解$R_j$ *？*

#### 1.3.2.2. 实验

测试两个线性分类器（多项式Logistic回归和线性支持向量机（SVM））和两个非线性分类器（具有RBF内核的SVM，具有不同层和隐藏单元大小的多层感知器）。为评估所有解码器之间的准确性是否有显着差异，使用统计上合适的F检验。令人惊讶的是，发现采样语言之间没有统计差异：非线性解码器没有比简单的线性解码器具有更高的精度。还在实验中观察到，随着MDFA M大小的增加，所有解码器的解码精度都以类似的方式降低。

![Compare-3.5](.\_v_images\Compare-3.5.png)

图3a显示多项式逻辑回归分类器的这种关系。

那么就想知道网络中R1的隐藏状态空间是如何组织的？与上述观察结果一致的一种假设是，受过训练的RNN反映了状态空间$Q^0$（图1）的粗粒度抽象，而不是MDFA状态本身。提出了一种简单的贪心算法来找到这样一个抽象映射，发现相对于随机选择的初始条件和模型参数，聚类的结果序列是稳定的。隐藏单元数量不同的识别器RNN会导致在**关键的前几个抽象中彼此一致的聚类序列**。（如果自动机算法任务定下来，实际上的decision structure网络结构大同小异，BNN或许二值化更接近布尔逻辑发家的计算机程序设计算法，但是就没有办法分开虚妄的实数，因为有定理说“任意一阶理论A都存在一个R的可数真子集S,使得S和R在理论A中不可分辨。”，而且实数完备性保证网络理论推导训练都能像插值似的迭代求值，简化问题，实数train出来的网络大行其道不无道理。但是要思考，一些任务问题下，是不是用BNN会不会也可以？）

图4：所有解码精度与粗糙度图的曲线下的平均归一化面积（图4a）（AUC）（类似于图8）。4b（右）：必须创建的平均粗糙度比

![Compare-4](.\_v_images\Compare-4.png)

图5：5a（左）：平均线性解码器测试准确度与粗糙度（施加*a*的次数）的函数，按MDFA中的节点数排序。5b（右）：平均过渡精度与粗糙度的关系，使用线性解码器时，按MDFA中的节点数进行排序。

![Compare-5](.\_v_images\Compare-5.png)

SIMPLE EMAIL语言的MDFA，树状图表示使用线性解码器时创建的抽象序列。显示初始抽象的是具有相同模式[ad] *的抽象。7b（底部）DATES语言的MDFA，树状图表示使用线性解码器时创建的抽象序列。显示初始抽象的是那些表示相同时间的状态的抽象。

在图7中有真实世界的解释，即在S两个正则表达式的聚类序列SIMPLE EMAIL和DATES分别识别简单的电子邮件和简单的日期语言。为了解释，图7b显示DATES语言，其聚类序列以树状图的形式叠加在MDFA上。可以以自上而下的方式读取树状图，该树状图显示MDFA状态的成员资格以及抽象序列，直到n = *M -1* 。

然后出现一个问题：应该如何选择正确的抽象级别n？

答案可以在相应的精度可以看出  *，*在图8 AL。

![Compare-6](.\_v_images\Compare-6.png)

随着n的增加和NFA状态总数的减少，线性解码（LDC）预测任务显然会变得更容易（当NFA状态数目$Q^{| Q | -1}$为1时，准确度为100％），因此重要的是要考虑如何选择最终分区中的抽象数量。通常为AL设置一个阈值，然后选择达到阈值精度所需的最小n。


#### 1.3.2.3. 结论

研究经过训练可识别常规形式语言的RNN如何以隐藏状态表示知识。

具体来说，询问是否可以将这种内部表示形式解码为能够准确识别该语言的规范的最小DFA，因此可以将其视为“基本事实”。已经表明，线性函数在执行这种解码时表现出色。然而，至关重要的是，该解码器不是将RNN的状态映射到MDFA状态，而是映射到通过将一小组MDFA状态聚类为“抽象”而获得的抽象状态。

总体而言，结果表明RNN所使用的内部表示与有限自动机之间存在很强的结构关系，并解释RNN识别形式语法结构的众所周知的能力。认为工作是研究神经网络如何学习形式逻辑概念的更大努力的基本步骤,计划探索形式语言的更复杂和更丰富的类，例如无上下文语言和递归枚举语言，以及它们的神经类似物。

### 1.3.3. Rule Extraction from Recurrent Neural Networks:A Taxonomy and Review

从递归神经网络（RNN）进行规则提取（RE）指的是找到底层RNN的模型，通常以有限状态机的形式出现，该模型可以将网络模仿到令人满意的程度，同时具有更加透明的优势。可以说，与其他或多或少的临时方法相比，RNN的RE允许对RNN进行更深入，更深刻的分析。RE可能使对RNN的理解介于介于作为一类计算设备的RNN的相当抽象的理论知识和RNN实例的定量性能评估之间。自1990年代初以来，从RNN提取规则的技术一直是活跃的领域。

本文回顾该开发的进度并对其进行详细分析。为构建调查并评估技术，已经开发出专门为此目的设计的分类法。而且，发现重要的开放研究问题，如果解决得当，可能会给该领域带来巨大的推动。在这篇综述中，对从离散时间递归神经网络（DTRNN，或简称RNN）中提取规则（或有限状态机）的技术进行综述。提出一种新的分类法，用于对现有技术进行分类，介绍这些技术，对其进行评估，并列出需要解决的公开研究问题的列表。

通过从RNN（以下称为RNN-RE）中提取规则，指的是找到并构建（最好是可理解的）形式化计算模型和能够令人满意地模拟RNN的机器的过程。

RNN和形式化的计算模型之间的联系几乎与RNN本身的研究一样古老，因为这些领域的起源在很大程度上是重叠的。神经网络的研究曾经与McCulloch和Pitts在神经系统理论工作的有限状态自动机的二进制递归网络实现中的计算研究相吻合。

从理论上讲，RNN与Turing等效，因此可以计算任何数字计算机可以计算的任何功能。但是也知道，让RNN执行所需的计算非常困难。这使处于知识真空的形式；知道RNN可能是功能强大的计算设备，并且也知道找到执行这些计算的RNN的实例化很可能是无法克服的障碍，但是没有办法有效地确定当前RNN的计算能力实例化。

从理论上讲，可以简单地评估不同RNN的性能，以解在多大程度上解决特定领域的学习问题。几乎所有在域上应用RNN的论文都进行此类研究，并且在某些情况下还提供更为系统的研究。但是，即使像评估RNN在特定域上的性能之类的简单操作也存在一些内在问题，因为评估程序的隐含方面可能会对估计的量化性能产生重大影响。

实际上，分析问题可能导致使用过于简单的模型（例如较小的网络和玩具问题域），而仅仅是为能够分析（或可视化）结果。可能会想知道有多少个只有两个或三个状态（或隐藏）节点的已发布网络具有特定的拓扑选择，以使其内部激活的绘制成为可能。需要做的是对RNN实例进行深入分析，以发现RNN实例的实际行为，而无需“手动”分析RNN行为的可视化。有效的规则提取技术可能是进行此类分析的最佳工具。

#### 1.3.3.1. 理论

自1990年代初以来，关于递归神经网络的文章很多，其中许多已经明确地处理RNN与状态机之间的联系。在理论上做出许多贡献，建立（模拟）RNN（或其他动力系统）与传统（离散）计算设备之间的联系。这项工作涵盖广泛的，非常有趣且重要的理论见解，但是在本文中，将不讨论这些理论问题，因为它们不是本次调查的重点，并且因为其中一些文章已经非常像调查本身，在实用方面，发现一些文章，描述将状态机转换为RNN（规则插入）或将RNN转换为状态机（规则提取）的技术。但是，本文仅涉及用于从RNN执行规则提取的算法。

总而言之，本文仅以RNN-RE技术为重点，但该领域与已经提到的领域密切相关。值得一提的是，作为技术的回顾，本回顾不是教程。有兴趣复制这些技术的读者应参考所引用的文章。

**RNN递归神经网络**

![rule-1](.\_v_images\rule-1.png)

**有限状态自动机**

从RNN中提取的规则几乎完全表示为FSM。

![rule-2](.\_v_images\rule-2.png)

### 1.3.4. **主要标准**

*规则类型* RNN-RE算法生成的规则是确定性，非确定性或随机性的FSM。它们也可以是Mealy或Moore格式。在规则类型分类中，还将区分机器（和底层RNN）是在字符串末尾（例如，FSA）是在生成二进制接受/拒绝决策，还是任务是在生成输出序列基于输入序列的符号（通常用于预测）。

*量化* 现有RNN-RE算法中变化最大的元素之一是状态空间量化方法。使用的方法示例包括层次聚类，矢量量化和自组织映射（有关详细讨论，请参见6.2节）。

*状态生成* 另一个重要标准是状态生成过程，该过程有两种基本方法：搜索和采样。这些将在算法描述中进一步描述。

*NetworkType和Domain*尽管本身不是提取算法的功能，但是将为每种提出的技术明确列出网络类型以及已使用每种RNN-RE算法的域。表达力基本上是由RE生成的规则类型，因此被规则类型标准所包含。

ADT确定四个基本类别:

•命题逻辑（即，*如果...则...否则）*

*•非常* 规逻辑（例如，模糊逻辑）

•一阶逻辑（即带有量词和变量的规则）

•有限状态机

来自RNN-RE算法的几乎所有规则都将属于最后一类。

ADT分类法的主要方面之一是半透明性，也就是说，规则提取算法在ANN内部“看”的程度在这里不太相关，因为它不是RNN-RE算法的显着特征。

ADT最初确定三种RE算法：

（1）分解算法，其中规则是在单个神经元的层次上建立的，然后进行组合；

（2）使用基础网络的黑匣子模型进行教学的方法；

（3）折衷算法，具有前面两种类型的方面。Tickle等还引入第四种中间类别，即成分组合，以适应所有RNN-RE算法，这是基于分析神经元的集合（即隐藏状态空间）来实现的。

### 1.3.5. **4种RNN-RE技术_**

尽管已经确定RE算法之间的一些共同特征，但是将它们划分为一组是一项艰巨的任务，因为有无数种方法可以做到。这些技术将主要按照时间顺序进行介绍。当较晚的技术与较早的技术相似时，将结合其前身进行介绍（尽管这种关系可能是由巧合的相似性构成的，而不是先前工作的直接延续）。

**Pre-RE Approaches**

为理解来自递归网络的FSM提取的根源，认识到在分析RNN的一些早期尝试中，在状态空间上使用聚类技术，并且发现与生成该语言的FSM的状态相对应的聚类（聚类仍然是FSM的核心问题之一），这一点很有用。 RNN对RE的研究）。在有关RNN的一些早期文章中，使用层次聚类分析（HCA）来分析RNN（Cleeremans等，1989； Servan-Schreiber等，1989，1991； Elman，1990）。作者发现，对于由小型FSM生成的字符串进行训练的网络，HCA可能会在状态空间中找到显然与语法状态相对应的聚类。

关于RNN的早期研究大多是基于明确基于FSM的问题集进行的，这可能会使随后的研究偏向于在网络内部寻找这些FSM。但是，对于某些成功的网络（例如，Servan-Schreiber等，1991），未找到直接对应于FSM状态的集群，该集群生成训练集语言。这意味着该网络可以替代该问题，但显然是正确的，该问题与预期的问题有所不同。这可能是由于以下事实：网络内部状态的群集不一定与对应的最小计算机的状态具有直接的一对一关系。后来表明，非最小机器通常是最初在RNN上使用RE时通过对状态空间进行聚类而最初提取的东西（Giles，Miller，Chen，Chen和Sun，1992年）。因此，大多数RNN-RE算法都包含FSM最小化。仅使用聚类（而不记录过渡）来分析RNN的基本问题是，没有可靠的方法来告诉聚类在时间上如何相互关联。[59)如果未找到完全相同的FSM，则可能无法使用原始FSM作为源来标记集群，因此会丢失集群的时间顺序。Elman（1990）也观察到这个问题：“状态之间的时间关系丢失。人们想知道状态之间的轨迹是什么样子。” 这个问题的解决导致从RNN提取FSM的发展。

![rule-3](.\_v_images\rule-3.png)

**Search in Equipartitioned State Space。** 吉列特算法将状态空间划分为大小相等的超立方体（即大状态），并通过进食进行广度优先搜索网络输入模式，直到没有新分区被访问为止。宏状态之间的转换（由输入模式引起）是提取机器的基础。搜索从网络的预定义初始状态开始，并在此微状态下测试所有可能的输入模式（请参见图3和4）。然后，将每个宏状态的第一个遇到的微状态用于诱导新状态。这保证确定性机器的提取，因为避免任何状态漂移，因为当重新进入已经访问过的分区时，搜索被修剪。然后，使用针对DFA的标准最小化算法将提取的自动机最小化。

表2总结该算法,该算法的中心参数是等分的量化度*q* 。



![rule-4](.\_v_images\rule-4.png)

图4：Giles等人的DFA提取算法示例中的两个最终迭代。注意，对应于节点3的宏状态可以根据微状态解释为接受状态*和*拒绝状态，但是该算法使用对第一个遇到的微状态的解释作为宏状态的解释。

![rule-5](.\_v_images\rule-5.png)



表2：在等分状态空间中通过搜索提取DFA的算法摘要。

![rule-6](.\_v_images\rule-6.png)

这种技术的一个明显问题是，最坏情况的簇数随状态节点N（*q N*）的数量呈指数增长。广度优先搜索所需的时间也会随着可能的输入符号的数量呈指数增长。但是，实际上，访问状态的数量远小于可能状态的数量。

这是最早的RNN-RE方法，也是应用最广泛的算法。随后的几乎所有提出新的RNN-RE技术的文章都引用Giles，Miller，Chen，Chen等人的文章。但是，这些文章常常互不引用，给人以第一印象，但实际情况却不那么丰富。因此，存在令人惊讶的各种RE方法，其中一些方法似乎彼此独立地开发。

 **Search in State Space Partitioned Through Vector Quantization**

Zeng等人提出一种简单的等分量化的替代方法使用*k-*均值算法对微状态进行聚类。聚类的中心，即模型向量，被用作广度优先搜索的基础；针对每个模型向量状态，使用所有输入符号对RNN进行测试

![rule-7](.\_v_images\rule-7.png)

![rule-8](.\_v_images\rule-8.png)

其他研究也遵循这种方法。但是，在这种专用RNN上开发的RE-RNN算法可能不适用于其他网络。Tickle等人描述可在现有网络上使用的RE技术（即通常不设计为易于分析）作为更具吸引力的技术。

在提出的基于搜索的方法中，重新进入分区是**修剪搜索**的基础。Alquezar和Sanfeliu以及Sanfeliu和Alquezar提出一种不同的修剪策略，他们选择使用域来确定搜索深度（该算法总结在表4中）。根据训练集中出现的正负字符串构建前缀树（参见图6）。前缀树仅包含训练集中存在的字符串。仅使用前缀树中的字符串来生成RNN的状态。

![rule-9](.\_v_images\rule-9.png)



**Sampling-Based Extraction of DFA** 于其在量化状态空间中进行搜索，不如记录RNN与数据和环境交互作用的活动。这样，可以将域视为将RNN的状态限制为相关状态的启发式方法。在开发用于RNN的RE技术之前，使用域对状态空间进行采样是进行RNN分析的最自然的方法。Watrous和Kuhn提出第一种基于对RNN进行采样的RE技术（请参见表5）。

![rule-10](.\_v_images\rule-10.png)

状态空间的量化基于将各个状态单元的激活划分为多个间隔。他们指出，这些间隔可以合并和拆分，以帮助提取最小确定性规则。但是，状态划分的过程描述得有些模糊，可能需要用户的干预。

![rule-11](.\_v_images\rule-11.png)

**随机机器提取。**状态空间的量化可能导致宏状态转换的不一致，这阻碍通过采样从RNN中提取确定性FSM（DFM）。分两个阶段：训练集抽样和自驱动RNN主要是二阶RNN。Tiiio和Vojtek（1998）提出一种从RNN中提取随机机器的算法。但是，这些抽取的机器不等同于Paz（1971）和Rabin（1963）定义的随机机器，因为在模型中不包括给定状态转移的输出的条件概率。提取的机器未对RNN的输出建模。该算法使用SOM对状态空间进行量化（如Tino和Sajda，1995年所做的那样）。状态的生成和状态转换分为两个阶段：预测试阶段（其中RNN是域驱动的）和自驱动阶段（其中RNN的输出用作下一个时间步的输入）（此RNN经过培训可以预测较长的符号序列）。在Tino和Koteles（1999）（在Tino，Dorffner，＆Schittenkopf，2000中进一步描述）中，

随机机器可以以新颖有趣的方式进行分析。例如，作者（Tiiio＆Vojtek，1998； Tiiio＆Kc）teles，1999）使用熵谱（Young＆Crutchfield，1993）将RNN生成的字符串的概率与原始RNN的字符串的概率进行比较。来源。结果很有趣，但是Tiio和Koteles（1999）以及Tiio和Vojtek（1998）没有迹象表明所提取的机器与网络的对应程度（即规则保真度）或它们在任何测试集上的推广程度如何（即规则的准确性）。

表9：用于提取随机机器的RNN-RE方法摘要。

![rule-12](.\_v_images\rule-12.png)

表10：在状态转换未纳入模型的情况下，通常从RNN中提取的与FSM不同的神经预测机（NPM）。

![rule-13](.\_v_images\rule-13.png)

**A Pedagogical Approach**

先前描述的所有算法都属于ADT的半透明分类的组成类别。据所知，只有一种使用Pedagogical 方法的算法。Vahed和Omlin使用一种机器学习方法，只需要输入和输出就可以提取机器。内部状态将被忽略（请参见表11中的摘要）。用于提取的数据基于直到给定长度的所有字符串。记录网络的输入和输出，并将其输入到多项式时间$Trakhtenbrot-Barzdin$算法。

还据报道，该算法比基于聚类的算法在返回正确的DFA方面更为成功。实际上，这似乎是唯一一篇描述不同RE技术的实验比较的文章。考虑到前缀树（请参见图6）可用，他们使用的机器学习算法确实具有多项式时间复杂性。但是，直到字符串长度*L*为止的前缀树的大小的复杂度为$O(n^L)$，其中*n*是符号数。结果，这种方法可能会遇到一些问题，这些问题会扩展到具有更多符号的更复杂的问题。

表11：在提取过程中不考虑RNN内部状态的唯一RNN-RE算法。

![rule-14](.\_v_images\rule-14.png)




#### 1.3.5.1. 结论

尽管取得许多成就，但RNN-RE研究社区似乎没有明显的共同方向（或明确的目标）。在大多数情况下，开发的算法似乎并不是基于先前的结果构建的，并且在使用RNN-RE算法处理更复杂的RNN和域方面似乎进展非常缓慢（如果有的话）。实际上，在后续工作中仅广泛使用一种算法，而且，它是第一个开发的RNN-RE算法。令人惊讶的是，它没有被任何更好的替代品。实际上，以后的算法可能会更好，但是仍然不如第一个频繁使用，并且几乎没有比较研究。

本文指出重要的研究问题，这些问题如果得到解决，将有助于推动该领域的发展。更重要的是，已经提出应该衡量该领域的任何进展的目标。需要进一步明确这些目标的构成，并且需要不断对其进行更新，但是主要问题是，逐渐消除缺乏共同目标的情况。自然而然的下一步是找到可以比较规则提取技术的标准化问题。优选地，标准化问题将不仅限于RNN问题，而且还包括其他动力学系统问题。

图8：理想的RNN-RE算法的执行时间，保真度和可理解性之间的关系，以及可能的规则逐步完善。可用时间越长，在高保真度和可理解性之间进行选择的自由度就越高。

![rule-15](.\_v_images\rule-15.png)

通常，RNN和神经网络都是模拟实体，因此只要拥有研究它们的工具，它们就可以“轻松学习”。而且这里回顾的算法可能为种子分析提供比以前更深入，更笼统的概念。一旦对网络的实际运行有更深入的解，更好的分析工具可能反过来帮助RNN研究更快地进行。在许多其他科学学科中，进行中的量子飞跃通常源于更复杂的分析工具和测量设备，它们产生与现有模型（异常）相冲突的定性新数据，最终可能导致科学革命。今天，对RNN在实践中的作用有深刻但部分矛盾的理论（即，用批判性眼神，从递归神经网络中提取规则似乎是另一个无限小子域中的一个无限小子域，因此提供有趣的科学结果的潜力非常有限。

但是，如果将来有一个放大RNN的显微镜，我会认为有充分的理由相信规则提取机制将是该显微镜的操作部分或透镜。就像现实世界的显微镜一样，这种RNN显微镜如果足够通用的话，将能够放大其他类型的动力系统和物理计算设备，从而在更广泛的意义上为科学界做出贡献。

## 1.4. NN学到的到底是什么决策结构？

《花书》没有免费午餐定理：在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。告诉没有万能算法可以解决任何问题。目标是理解什么样的分布与人工智能获取经验的 ‘‘真实世界’’ 相关，什么样的学习算法在关注的数据生成分布上效果最好。

不像文法自动机倾向于具体算法函数，神经网络学的是逼近这个算法要解决问题的概率分布的很多种映射函数的其中一种(如XOR) 欧氏空间之间的非线性映射，或者说学到的是解空间。因此如何用**映射表示概率分布**成为研究重点。这里一个强化学习DQN的目标是学习到一个离开房子的[策略policy](https://blog.csdn.net/itplus/article/details/9361915),注意不是唯一解控制流图，观察实际上train出来右边的解图结构中，房间三号,S3到S1是一种解，S3到S4另一个可行解。观察链接中的解图，初始点I --> G目标点 竟然有两个解图！写成确定性状态转移决策算法的话，位于状态3号既可以去1也可以4，概率性决策选一个就能解决问题！

满足问题要求的数据分布有多个解答，把他俩函数映射关系合起来看作一个解空间，概率性选一个学出来就可以！

这配合异或门的多解例子，进一步说明神经网络的求解观点是：不需要学出来一个确定性算法/状态迁移系统/petri网络，只需要学习适合任务要求概率分布的决策结构decision structe! 

时髦点的话来说就是：设计的神经网络结构需要能够从数据中学习隐藏的流形结构，这样训练就能学习到 特定任务在流形上的概率分布。

换言之，核心假设是”需要求解的任务数据冥冥中满足特定规律“。

天地有正气，杂然赋流形。

这点引用顾险峰的话就是”高维数据分布在低维流形附近，流形中具有特定概率分布，深度学习网络具有强大的逼近非线性映射的能力。有时候，神经网络毕竟的不是函数或映射，而是概率性分布。更为重要的，逼近概率分布比逼近映射要容易得多。在理想情况下，即逼近误差为零的情形，如果神经网络逼近一个映射，那么解空间只包含一个映射；如果神经网络逼近一个概率分布，那么解空间包含无穷个映射，这些映射的差别构成一个无穷维李群。“

通俗解释就是：如果说：网络可以学习计算图算法等“确定性函数”和神经网络结构的同构关系这点,可能过分强调确定性运算状态迁移关系，实际上我认为应该强调激活函数的非线性决策集群的投票行为，才是真正解决问题实现的非线性decision structure（概率分布多种解决结构decision structure的一种就行）。

最近看到一个有趣的[谷歌最新提出无需卷积、注意力 ，纯MLP构成的视觉架构！网友：MLP is All You Need](https://zhuanlan.zhihu.com/p/370789914)（*又回到最初的起点*记忆中妳青涩的脸终于来到这一天......），已经用烂了的套路虽然好用得人心，但是创新可能就是掌握最基本的设计动机出发点，针对特定问题设计结构。神经网络的非线性拟合能力太强，进而用来近似特定任务对应“冥冥中数据分布规律”的求解算法decision structure可能真的有很多很多中“殊途同归”的道路，现在发展的很多套路操作可能真的只是历史遗留问题---人们总是跟着效果好的做下去，哪怕有可能这效果好只是暂时的“We should forget about small efficiencies, say about 97% of the time: **premature optimization is the root of all evil**. Yet we should not pass up our opportunities in that critical 3%.”。越是底层的知识，越是需要负重前行。


# 2. 文献

[^形式化方法CCF]: 形式化方法的研究进展与趋势 https://dl.ccf.org.cn/books/detail.html?_ack=1&id=4138192862726144
[^人工智能形式化CCF]: 人工智能系统的形式化发展与趋势 https://dl.ccf.org.cn/books/detail.html?_ack=4&id=5161520744974336
[^SMT-LIB]: SMT-LIB http://smtlib.cs.uiowa.edu/benchmarks.shtml 
