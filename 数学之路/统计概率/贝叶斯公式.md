# 贝叶斯公式







数学大神这个贝叶斯定理怎么解答? \- Tridu的回答 - 知乎 https://www.zhihu.com/question/412470730/answer/1391462141







我想回答这个问题：

>一辆出租车肇事后逃逸。
这座城市有两家出租车公司，其中一家车是绿色的，另一家是蓝色的。
知道以下数据：
这座城市85％的出租车是绿色的，15％是蓝色的。
一位目击人辨认出那辆肇事出租车是蓝色的。当晚警察在出事地点对证人的证词进行了测试，得出结论：目击者当时能正确辨认出这两种颜色的概率是80％，错误概率是20％。
问：这场事故的出租车是蓝色而不是绿色的概率是多少

**先验概率** ： X={实际是绿车 ，实际是蓝车}={0.85，0.15}

| 似然率 |      |      |
| -------- | ---- | ---- |
|            | X=实际是绿车 | X=实际是蓝车  |
| Y=认为是绿车  | 0.80认对   |      0.20认错            |
| Y=认为是蓝车  | 0.20认错      | 0.80认对            |





第一种观测顺序

```mermaid
graph LR;
　　Origin-->|0.85|X=实际是绿车;
　　Origin-->|0.15|X=实际是蓝车;
　　X=实际是绿车 -->|0.8|Y=认为是绿车0.71 ;
　　X=实际是绿车 -->|0.2|Y=认为是蓝车0.29 ;
　　X=实际是蓝车 -->|0.2|Y=认为是绿车0.71 ;
　　X=实际是蓝车 -->|0.8|Y=认为是蓝车0.29 ;
```

根据第一张图求出认为是蓝车全概率$0.85*0.2+0.15*0.8=0.29000$(不随观测顺序改变，全局下看Y事件的独立概率）

根据第一张图认为是绿车全概率$0.85*0.8+0.15*0.2=0.71000$(不随观测顺序改变，全局下看Y事件的独立概率）




**后验概率**$P(X=实际是蓝车|Y=认为是蓝车)$
$=\frac{P(Y=认为是蓝车|X=实际是蓝车)*P(X=实际是蓝车)}{P(Y=认为是蓝车|X=实际是蓝车)*P(X=实际是蓝车)+P(Y=认为是蓝车|X=实际是绿车)*P(X=实际是绿车)}$
$=\frac{0.8*0.15}{0.8*0.15+0.2*0.85} =\frac{0.12}{0.29}= 0.41379$






换一种观测顺序，对应这张图：

```mermaid
graph LR;
　　Origin-->|第一张图蓝车全概率|Y=认为是蓝车;
　　Origin-->|第一张图绿车全概率|Y=认为是绿车;
　　Y=认为是蓝车 -->| 0.41379|X=实际是蓝车;
　　Y=认为是蓝车 -->|贝叶斯可求|X=实际是绿车;
　　Y=认为是绿车 -->|贝叶斯可求|X=实际是蓝车;
　　Y=认为是绿车 -->|贝叶斯可求|X=实际是绿车;
```

----------------------


如果觉得上一张图看得不够清楚，可以把它画完整。

$P(X=实际是绿车|Y=认为是蓝车)$
$=\frac{P(Y=认为是蓝车|X=实际是绿车)*P(X=实际是绿车)}{P(Y=认为是蓝车|X=实际是蓝车)*P(X=实际是蓝车)+P(Y=认为是蓝车|X=实际是绿车)*P(X=实际是绿车)}$
$=\frac{0.2*0.85}{0.2*0.85+0.8*0.15}= \frac{0.17}{0.29}=0.58621$

$P(X=实际是蓝车|Y=认为是绿车)$
$=\frac{P(Y=认为是绿车|X=实际是蓝车)*P(X=实际是蓝车)}{P(Y=认为是绿车|X=实际是蓝车)*P(X=实际是蓝车)+P(Y=认为是绿车|X=实际是绿车)*P(X=实际是绿车)}$
$=\frac{0.2*0.15}{0.2*0.15+0.8*0.85}=0.03/0.71=0.042254$


$P(X=实际是绿车|Y=认为是绿车)$
$=\frac{P(Y=认为是绿车|X=实际是绿车)*P(X=实际是绿车)}{P(Y=认为是绿车|X=实际是蓝车)*P(X=实际是蓝车)+P(Y=认为是绿车|X=实际是绿车)*P(X=实际是绿车)}$
$=\frac{0.8*0.85}{0.2*0.15+0.8*0.85}=0.68/0.71=0.95775$


```mermaid
graph LR;
　　Origin-->|0.29000|Y=认为是蓝车;
　　Origin-->|0.71000|Y=认为是绿车;
　　Y=认为是蓝车 -->| 12/29 |X=实际是蓝车;
　　Y=认为是蓝车 -->|17/29|X=实际是绿车;
　　Y=认为是绿车 -->|3/71|X=实际是蓝车;
　　Y=认为是绿车 -->|68/71|X=实际是绿车;
```



如果觉得推导贝叶斯公式的正确性代数证明验证太抽象舞法理解，可以感受一下数学理论合理的“自洽”。单独观测"X,Y"事件是独立的，现实世界的独立事件发生概率p(X)p(Y) 和我们观察者 “先考虑X事件（第一张图）”  或者“先考虑Y事件(第二张图)”无关！全概率公式求解P(X={实际是蓝车，实际是绿车})

实际是蓝车
$0.29*12/29+0.71*3/71=0.15$


实际是绿车
 $0.29*17/29+0.71*68/71=0.85$


没错，和第一张图的X独立概率是一样的。就是换个顺序观测问题，两个独立变量XY，被观测顺序不会影响他们的全局观测下的独立概率(验证一下全概率公式就知道是一致的)。！其实代数证明显然也是一样的。

贝叶斯规则就是这样，修改题目，如果题目多几个目击证人，就(n-1)次算出来的后验概率当作第n次计算的先验概率，

```mermaid
graph LR;
　　Y1=认为是蓝车 -->| 12/29 |X=实际是蓝车;
　　Y1=认为是蓝车 -->|17/29|X=实际是绿车;
　　X=实际是绿车 -->|第二个证人正确辨色概率|Y2=认为是绿车 ;
　　X=实际是绿车 -->|第二个证人错误辨色概率|Y2=认为是蓝车 ;
　　X=实际是蓝车 -->|第二个证人错误辨色概率|Y2=认为是绿车 ;
　　X=实际是蓝车 -->|第二个证人正确辨色概率|Y2=认为是蓝车 ;
```

然后几次观测迭代更新“后验概率”。或者可以理解第二张图因为观测到“Y1=认为是蓝车”为剪枝后的“更有可能发生的情况”。


比如有很多目击证人，口供序列{蓝，绿，绿，...，绿},把P(Y=真实是绿车)的概率画出图来，先降后升。

接下来，开始学习(ML的)概率图模型吧。




# LearningProbabilisticGraphicalModelsInR

LearningProbabilisticGraphicalModelsInR有例题解析






github.com/PacktPublishing/Learning-Probabilistic-Graphical-Models-in-R
随书代码

《概率图模型--基于R语言》




随手找个例子：

```mermaid
graph LR;
　　Origin-->|0.6|X1_0.6;
　　Origin-->|0.4|X2_0.4;
　　X1_0.6-->|0.7|Y1_0.46;
　　X1_0.6-->|0.1|Y2_0.38;
　　X1_0.6-->|0.1|Y3_0.16;
　　X2_0.4-->|0.1|Y1_0.46;
　　X2_0.4-->|0.8|Y2_0.38;
　　X2_0.4-->|0.1|Y3_0.16;
```

$P(X|Y)=\frac{P(Y|X)P(X)}{P(Y)}=\frac{P(Y|X)P(X)}{\sum_x P(Y|X)*P(X)}$







全概率Y1 0.46 Y2 0.38 Y3 0.16


贝叶斯公式可以推导：用定义式$P(X|Y)=\frac{P(x,y)}{P(y)}$和$P(Y|X)=\frac{P(x,y)}{P(x)}$

推导出**一般形式的贝叶斯公式**$P(X|Y)=\frac{P(Y|X)P(X)}{P(Y)}$

实际问题中，一般已知第一张图【X,Y,...,Z随便给定一个顺序（已经求解出），有可能我们一会儿关心（要求解）Y,X,...,Z顺序的条件概率,一会儿关心（要求解）X,Z,...,Y顺序的条件概率，贝叶斯公式能帮到你利用第一张图求解其他而不需要详细慢慢画出来第二三四n张图。换句话说就是，通过对变量X的观察，得到其他变量的后验概率分布等信息。而概率图模型更是666】，

现在欲求解“给定Y下X的**后验分布**”$P(X|Y)${第二张未知待求图的条件概率}，$P(X)$**先验分布**{全局下X独立事件发生概率},$P(Y|X)$**似然率**{第一张图的条件概率，X事件发生条件下Y的概率}，$P(Y)$**归一化因子**{全局下Y独立事件发生概率,在第一张已知数据的图中是全概率公式求解的,边缘化$P(y)=\sum_x P(x,y)=\int_x P(X,y)dx$消除联合概率分布下单个X变量的边缘化}


定义式$P(x,y)=P(y|x)*P(x)$和边缘化$P(y)=\sum_x P(x,y)=\int_x P(X,y)dx$消除联合概率分布下单个X变量的边缘化
推导出来$P(y)=\sum_x P(X,Y)=\sum_ P(Y|X)*P(x)$

把上述公式代入一般形式的贝叶斯公式改写成**更方便实用的贝叶斯公式形式**：

$P(X|Y)=\frac{P(Y|X)P(X)}{P(Y)}=\frac{P(Y|X)P(X)}{\sum_x P(Y|X)*P(X)}$,这样每个数都能直接从第一张图(已知)看出来，求给定Y下X的后验分布的过程中，连归一因子P(Y)都没必要拎出来算就有结果。只需要看图查出来：$P(X)$**先验分布**{全局下X独立事件发生概率},$P(Y|X)$**似然率**{第一张图的条件概率，X事件发生条件下Y的概率}，






其实贝叶斯公式的意义就是:*在不求解下面图的情况下* 快速算出来更换顺序后的**条件概率**（毕竟变量多的话，求起来还挺麻烦的）多少？


条件概率

Y1


$P(X_1|Y_1)=\frac{P(X_1)P(Y_1|X_1)}{P(X_1)P(Y_1|X_1)+P(X_2)P(Y_1|X_2)}=\frac{0.6*0.7}{0.6*0.7+0.4*0.1}=\frac{21}{23}$

$P(X_2|Y_1)=\frac{P(X_2)P(Y_1|X_1)}{P(X_1)P(Y_1|X_1)+P(X_2)P(Y_1|X_2)}=\frac{0.4*0.1}{0.6*0.7+0.4*0.1}=\frac{2}{23}$

Y2

$P(X_1|Y_2)=\frac{P(X_1)P(Y_2|X_1)}{P(X_1)P(Y_2|X_1)+P(X_2)P(Y_2|X_2)}=\frac{0.6*0.1}{0.6*0.1+0.4*0.8}=\frac{3}{19}$

$P(X_2|Y_2)=\frac{P(X_2)P(Y_2|X_1)}{P(X_1)P(Y_2|X_1)+P(X_2)P(Y_2|X_2)}=\frac{0.4*0.8}{0.6*0.1+0.4*0.8}=\frac{16}{19}$

Y3

$P(X_1|Y_3)=\frac{P(X_1)P(Y_3|X_1)}{P(X_1)P(Y_3|X_1)+P(X_2)P(Y_3|X_2)}=\frac{0.6*0.2}{0.6*0.2+0.4*0.1}=0.75$

$P(X_2|Y_3)=\frac{P(X_2)P(Y_3|X_1)}{P(X_1)P(Y_3|X_1)+P(X_2)P(Y_3|X_2)}=\frac{0.4*0.1}{0.6*0.2+0.4*0.1}=0.25$




对应的不同顺序的图就是下面这图

把"X,Y"事件记作"A,B"事件{方便往下列举很多}，A,B事件是独立的，现实世界的独立事件发生概率p(X)p(Y) 和我们观察者 “先考虑A上图”  或者“先考虑B事件发生”无关！

举例A1-A2和B1-B2,...,Z1-Z2,ZA1-ZA2,...就是当P(A1)=0.5=P(A2)拍硬币的伯努利事件。

举例，A1-A6和B1-B6,...,Z1-Z6,ZA1-ZA6,...就是掷骰子事件的不断重复）：

```mermaid
graph LR;
　　Origin-->|0.46|Y1_0.46;
　　Origin-->|0.38|Y2_0.38;
　　Origin-->|0.16|Y3_0.16;
　　Y1_0.46-->|21/23|X1_0.6;
　　Y1_0.46-->|2/23|X2_0.4;
　　Y2_0.38-->|3/19|X1_0.6;
　　Y2_0.38-->|16/19|X2_0.4;
　　Y3_0.16-->|0.75|X1_0.6;
　　Y3_0.16-->|0.25|X2_0.4;
```


全概率公式验证（同一个问题不同分发的X1=0.6 ,X2=0.4全局下看，概率必须一致，讲道理的数学“自洽”）

```octave
octave:1> 0.46*21/23+0.38*3/19+0.16*0.75
ans =  0.60000
octave:2> 0.46*2/23+0.38*16/19+0.16*0.25
ans =  0.40000
```

## A-Z-ZA-...很多事件叫做：联合概率分布





 边缘化Marginalization



$P(y)=\sum_y P(x,y)=\int_y P(X,y)dy$

## 贝叶斯公式理解




欲求解“给定Y下X的**后验分布**”$P(X|Y)${第二张未知待求图的条件概率}，$P(X)$**先验分布**{全局下X独立事件发生概率},$P(Y|X)$**似然率**{第一张图的条件概率，X事件发生条件下Y的概率}，$P(Y)$**归一化因子**{全局下Y独立事件发生概率,在第一张已知数据的图中是全概率公式求解的,边缘化$P(y)=\sum_x P(x,y)=\int_x P(X,y)dx$消除联合概率分布下单个X变量的边缘化}

**更方便实用的贝叶斯公式形式**：

$P(X|Y)=\frac{P(Y|X)P(X)}{P(Y)}=\frac{P(Y|X)P(X)}{\sum_x P(Y|X)*P(X)}$,

改写为：$X->\theta,Y->D$,

$P(\theta|D)=\frac{P(D|\theta)P(\theta)}{P(D)}=\frac{P(D|\theta)P(\theta)}{\sum_x P(D|\theta)*P(\theta)}$

$P(\theta)$先验概率：初始Belief,新观察之前对D的认知。
$P(D|\theta)$给定$\theta$下的似然率：我可以**生成**什么数据D。即对所有的$\theta,D$的概率是多少。参考
>$P(Y|X)$**似然率**{已知图的条件概率，X事件发生条件下Y的概率}

知道：先验概率初始认知$\theta$的前提条件下D事件发生的条件概率

$P(\theta|D)$后验概率：观察到D之后的新Belief


总结，新的先验是当前计算出来的后验概率，重复上面的公式就能不断更新Belief


很多变量的概率图模型中，划分因子图，已经观测得到独立变量顺序第一张图，不用重新画图就能得到变动被观测独立变量顺序的第二张图。



找到P13《标题的书》中有一个电灯泡的例题，可以辅助理解更新过程

机器先验分布M={working0.99,broken0.01}

灯泡L={good,bad}

下列是P(L|M)似然率

| likehood |      |      |
| -------- | ---- | ---- |
|          | good | bad  |
| working  | 0.99 | 0.1  |
| broken   | 0.6  | 0.4  |

后验分布要用公式求解

新的先验是当前计算出来的后验概率，迭代。。。。。。

P14计算出来的新的先验应该是71%为什么它下一步直接是50%？


实际画出来的图很形象很靠谱，随着bad,bad,bad会迅速变坏

随着bad,good,good,good,good,good,good...good机器正常工作的概率先降后升




-------------------
卡尔曼滤波是根据测量到的数据和噪声的概率分布计算最可能的真实值
贝叶斯推理是根据新数据可用的时候（结合旧的Belief数据概率）更新改进计算新的事实和结论








# index
1. 概率推理--基本建模方法
2. 精确推理--图的组合和模块的查询
3. 学习参数--极大似然拟合学习
4. 贝叶斯建模--基础模型，更复杂，用自适应算法查询和拟合
5. 近似推断 概率图模型第二种推断方法，主要的采用方法“MCMC马尔可夫蒙特卡洛”
6. 贝叶斯建模--线性模型  
7. 概率混合模型




























































































































































































































































































































